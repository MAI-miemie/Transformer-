{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba14dd96",
   "metadata": {},
   "source": [
    "# 名字生成任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a90220",
   "metadata": {},
   "source": [
    "## 导入库和基础组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e67ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试名字生成的基础组件:\n",
      "位置编码输入形状: torch.Size([2, 5, 8])\n",
      "位置编码输出形状: torch.Size([2, 5, 8])\n",
      "自注意力输入形状: torch.Size([2, 5, 8])\n",
      "自注意力输出形状: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# 位置编码\n",
    "class NamePositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# 多头自注意力机制\n",
    "class NameSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim_qk = qk_dim // heads\n",
    "        self.head_dim_v = embed_size // heads\n",
    "\n",
    "        assert qk_dim % heads == 0, \"qk_dim 必须能被 heads 整除\"\n",
    "        assert embed_size % heads == 0, \"embed_size 必须能被 heads 整除\"\n",
    "\n",
    "        self.query = nn.Linear(embed_size, qk_dim)\n",
    "        self.key = nn.Linear(embed_size, qk_dim)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, query, key=None, value=None, mask=None):\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        N = query.shape[0]\n",
    "        query_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        value_len = value.shape[1]\n",
    "\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        Q = Q.view(N, query_len, self.heads, self.head_dim_qk).transpose(1, 2)\n",
    "        K = K.view(N, key_len, self.heads, self.head_dim_qk).transpose(1, 2)\n",
    "        V = V.view(N, value_len, self.heads, self.head_dim_v).transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim_qk)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        out = torch.matmul(attention, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.embed_size)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# 测试位置编码和自注意力\n",
    "print(\"测试名字生成的基础组件:\")\n",
    "pe = NamePositionalEncoding(embed_size=8, max_len=10)\n",
    "test_input = torch.randn(2, 5, 8)\n",
    "output = pe(test_input)\n",
    "print(f\"位置编码输入形状: {test_input.shape}\")\n",
    "print(f\"位置编码输出形状: {output.shape}\")\n",
    "\n",
    "attention = NameSelfAttention(embed_size=8, heads=4, qk_dim=8)\n",
    "test_input = torch.randn(2, 5, 8)\n",
    "output = attention(test_input)\n",
    "print(f\"自注意力输入形状: {test_input.shape}\")\n",
    "print(f\"自注意力输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546889c2",
   "metadata": {},
   "source": [
    "## 前馈神经网络和Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8f8118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试名字生成的前馈网络:\n",
      "前馈网络输入形状: torch.Size([2, 5, 8])\n",
      "前馈网络输出形状: torch.Size([2, 5, 8])\n",
      "\n",
      "测试名字生成的Decoder Block:\n",
      "Decoder Block输入形状: torch.Size([2, 5, 8])\n",
      "Decoder Block输出形状: torch.Size([2, 5, 8])\n",
      "Mask形状: torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 前馈神经网络\n",
    "class NameFeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Decoder Block\n",
    "class NameDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim, ff_hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = NameSelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feed_forward = NameFeedForward(embed_size, ff_hidden_size, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        _x = self.self_attention(query=x, key=None, value=None, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(_x))\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        _x = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(_x))\n",
    "\n",
    "        return x\n",
    "\n",
    "# 测试前馈网络和Decoder Block\n",
    "print(\"测试名字生成的前馈网络:\")\n",
    "ffn = NameFeedForward(embed_size=8, hidden_dim=16, dropout=0.1)\n",
    "test_input = torch.randn(2, 5, 8)\n",
    "output = ffn(test_input)\n",
    "print(f\"前馈网络输入形状: {test_input.shape}\")\n",
    "print(f\"前馈网络输出形状: {output.shape}\")\n",
    "\n",
    "print(\"\\n测试名字生成的Decoder Block:\")\n",
    "decoder_block = NameDecoderBlock(embed_size=8, heads=4, qk_dim=8, ff_hidden_size=16, dropout=0.1)\n",
    "test_input = torch.randn(2, 5, 8)\n",
    "# 创建下三角mask\n",
    "mask = torch.tril(torch.ones((1, 5, 5)))\n",
    "output = decoder_block(test_input, mask)\n",
    "print(f\"Decoder Block输入形状: {test_input.shape}\")\n",
    "print(f\"Decoder Block输出形状: {output.shape}\")\n",
    "print(f\"Mask形状: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb52561",
   "metadata": {},
   "source": [
    "## 名字生成的数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75176534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试名字生成的数据预处理函数:\n",
      "测试字典: ['王', '李', '张', '刘', '陈', '伟', '娜', '强', '芳', '明']\n",
      "测试名字: ['王伟', '李娜', '张强']\n",
      "'王'在字典中的索引: 0\n",
      "名字'王伟'的索引序列: [0, 5]\n",
      "输入张量形状: torch.Size([2, 3, 10])\n",
      "实际长度: [2 2 2]\n",
      "目标张量形状: torch.Size([2, 3])\n",
      "第一个名字的输入:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "第一个名字的目标:\n",
      "tensor([5, 9])\n"
     ]
    }
   ],
   "source": [
    "# 名字生成的数据预处理函数\n",
    "def char2index(char, dict):\n",
    "    return dict.index(char)\n",
    "\n",
    "def name2index(name, dict):\n",
    "    return [char2index(char, dict) for char in name]\n",
    "\n",
    "def names2tensor(names, dict, charset_size):\n",
    "    n = len(names)\n",
    "    lens = [len(name) for name in names]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    tensor = torch.zeros(max_len, n, charset_size)\n",
    "    target = torch.zeros(max_len, n, dtype=int) + charset_size - 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        for j in range(len(name)):\n",
    "            tensor[j, i, char2index(name[j], dict)] = 1\n",
    "            if j < len(name) - 1:\n",
    "                target[j, i] = char2index(name[j + 1], dict)\n",
    "    \n",
    "    return tensor, np.array(lens), target\n",
    "\n",
    "# 测试名字生成的数据预处理函数\n",
    "print(\"测试名字生成的数据预处理函数:\")\n",
    "test_dict = ['王', '李', '张', '刘', '陈', '伟', '娜', '强', '芳', '明']\n",
    "test_names = [\"王伟\", \"李娜\", \"张强\"]\n",
    "\n",
    "print(f\"测试字典: {test_dict}\")\n",
    "print(f\"测试名字: {test_names}\")\n",
    "\n",
    "char_idx = char2index('王', test_dict)\n",
    "print(f\"'王'在字典中的索引: {char_idx}\")\n",
    "\n",
    "name_indices = name2index(\"王伟\", test_dict)\n",
    "print(f\"名字'王伟'的索引序列: {name_indices}\")\n",
    "\n",
    "input_tensor, actual_len, target_tensor = names2tensor(test_names, test_dict, len(test_dict))\n",
    "print(f\"输入张量形状: {input_tensor.shape}\")\n",
    "print(f\"实际长度: {actual_len}\")\n",
    "print(f\"目标张量形状: {target_tensor.shape}\")\n",
    "print(f\"第一个名字的输入:\\n{input_tensor[:, 0, :]}\")\n",
    "print(f\"第一个名字的目标:\\n{target_tensor[:, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0db7b",
   "metadata": {},
   "source": [
    "## Transformer Decoder和名字生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c1b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试名字生成的Transformer Decoder:\n",
      "Decoder输入形状: torch.Size([2, 5])\n",
      "Decoder输出形状: torch.Size([2, 5, 100])\n",
      "\n",
      "测试名字生成器:\n",
      "生成器输入形状: torch.Size([2, 5])\n",
      "生成器输出形状: torch.Size([2, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "# Transformer Decoder\n",
    "class NameTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, qk_dim,\n",
    "                 ff_hidden_size, dropout=0.1, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_encoding = NamePositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            NameDecoderBlock(embed_size, heads, qk_dim, ff_hidden_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt_mask=None):\n",
    "        out = self.word_embedding(x)\n",
    "        out = self.position_encoding(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, tgt_mask)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# 名字生成器\n",
    "class NameGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, qk_dim, \n",
    "                 ff_hidden_size, dropout=0.1, max_length=100):\n",
    "        super().__init__()\n",
    "        self.decoder = NameTransformerDecoder(\n",
    "            vocab_size, embed_size, num_layers, heads, qk_dim, \n",
    "            ff_hidden_size, dropout, max_length\n",
    "        )\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def forward(self, x, tgt_mask=None):\n",
    "        return self.decoder(x, tgt_mask)\n",
    "    \n",
    "    def generate_name(self, start_char, dict, max_len=10, temperature=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # 初始化输入\n",
    "            if start_char in dict:\n",
    "                start_idx = dict.index(start_char)\n",
    "            else:\n",
    "                start_idx = 0\n",
    "            \n",
    "            input_seq = [start_idx]\n",
    "            current_input = torch.tensor([input_seq], dtype=torch.long)\n",
    "            \n",
    "            for _ in range(max_len - 1):\n",
    "                # 创建mask\n",
    "                seq_len = current_input.size(1)\n",
    "                mask = torch.tril(torch.ones((1, seq_len, seq_len)))\n",
    "                \n",
    "                # 前向传播\n",
    "                output = self(current_input, mask)\n",
    "                next_token_logits = output[0, -1, :] / temperature\n",
    "                \n",
    "                # 采样下一个token\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # 检查是否结束\n",
    "                if next_token == len(dict):  # EOS token\n",
    "                    break\n",
    "                \n",
    "                input_seq.append(next_token)\n",
    "                current_input = torch.tensor([input_seq], dtype=torch.long)\n",
    "            \n",
    "            # 转换为字符\n",
    "            generated_name = ''.join([dict[idx] for idx in input_seq])\n",
    "            return generated_name\n",
    "\n",
    "# 测试Transformer Decoder和名字生成器\n",
    "print(\"测试名字生成的Transformer Decoder:\")\n",
    "decoder = NameTransformerDecoder(vocab_size=100, embed_size=8, num_layers=2, heads=4, \n",
    "                               qk_dim=8, ff_hidden_size=16, dropout=0.1, max_length=10)\n",
    "test_input = torch.randint(0, 100, (2, 5))\n",
    "mask = torch.tril(torch.ones((1, 5, 5)))\n",
    "output = decoder(test_input, mask)\n",
    "print(f\"Decoder输入形状: {test_input.shape}\")\n",
    "print(f\"Decoder输出形状: {output.shape}\")\n",
    "\n",
    "print(\"\\n测试名字生成器:\")\n",
    "generator = NameGenerator(vocab_size=100, embed_size=8, num_layers=2, heads=4, \n",
    "                         qk_dim=8, ff_hidden_size=16, dropout=0.1, max_length=10)\n",
    "test_input = torch.randint(0, 100, (2, 5))\n",
    "output = generator(test_input, mask)\n",
    "print(f\"生成器输入形状: {test_input.shape}\")\n",
    "print(f\"生成器输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adb4ab",
   "metadata": {},
   "source": [
    "## 名字生成的数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0be79f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试名字生成的数据预处理函数:\n",
      "测试字典: ['王', '李', '张', '刘', '陈', '伟', '娜', '强', '芳', '明']\n",
      "测试名字: ['王伟', '李娜', '张强']\n",
      "'王'在字典中的索引: 0\n",
      "名字'王伟'的索引序列: [0, 5]\n",
      "输入张量形状: torch.Size([2, 3, 10])\n",
      "实际长度: [2 2 2]\n",
      "目标张量形状: torch.Size([2, 3])\n",
      "第一个名字的输入:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "第一个名字的目标:\n",
      "tensor([5, 9])\n"
     ]
    }
   ],
   "source": [
    "# 名字生成的数据预处理函数\n",
    "def name_gen_char2index(char, dict):\n",
    "    return dict.index(char)\n",
    "\n",
    "def name_gen_name2index(name, dict):\n",
    "    return [name_gen_char2index(char, dict) for char in name]\n",
    "\n",
    "def name_gen_names2tensor(names, dict, charset_size):\n",
    "    n = len(names)\n",
    "    lens = [len(name) for name in names]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    tensor = torch.zeros(max_len, n, charset_size)\n",
    "    target = torch.zeros(max_len, n, dtype=int) + charset_size - 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        for j in range(len(name)):\n",
    "            tensor[j, i, name_gen_char2index(name[j], dict)] = 1\n",
    "            if j < len(name) - 1:\n",
    "                target[j, i] = name_gen_char2index(name[j + 1], dict)\n",
    "    \n",
    "    return tensor, np.array(lens), target\n",
    "\n",
    "# 测试名字生成的数据预处理函数\n",
    "print(\"测试名字生成的数据预处理函数:\")\n",
    "test_dict = ['王', '李', '张', '刘', '陈', '伟', '娜', '强', '芳', '明']\n",
    "test_names = [\"王伟\", \"李娜\", \"张强\"]\n",
    "\n",
    "print(f\"测试字典: {test_dict}\")\n",
    "print(f\"测试名字: {test_names}\")\n",
    "\n",
    "char_idx = name_gen_char2index('王', test_dict)\n",
    "print(f\"'王'在字典中的索引: {char_idx}\")\n",
    "\n",
    "name_indices = name_gen_name2index(\"王伟\", test_dict)\n",
    "print(f\"名字'王伟'的索引序列: {name_indices}\")\n",
    "\n",
    "input_tensor, actual_len, target_tensor = name_gen_names2tensor(test_names, test_dict, len(test_dict))\n",
    "print(f\"输入张量形状: {input_tensor.shape}\")\n",
    "print(f\"实际长度: {actual_len}\")\n",
    "print(f\"目标张量形状: {target_tensor.shape}\")\n",
    "print(f\"第一个名字的输入:\\n{input_tensor[:, 0, :]}\")\n",
    "print(f\"第一个名字的目标:\\n{target_tensor[:, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d7a95",
   "metadata": {},
   "source": [
    "## 名字生成的数据准备函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e204e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试名字生成的数据准备函数:\n",
      "数据大小: 72247\n",
      "字典大小: 50\n",
      "字符集大小: 51\n",
      "前10个常用字: ['尔', '斯', '拉', '克', '特', '德', '尼', '夫', '利', '科']\n",
      "前5个名字示例: ['阿阿巴伊', '阿阿卡伊', '阿阿拉尔', '阿阿内尔', '阿阿韦尔迪']\n"
     ]
    }
   ],
   "source": [
    "# 名字生成的数据准备函数\n",
    "def prepare_name_data():\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(\"English_Cn_Name_Corpus（48W）.txt\", header=None, names=[\"name\"], skiprows=2)\n",
    "    names = df[\"name\"].values\n",
    "    \n",
    "    # 计算单字频率\n",
    "    chars = [list(name) for name in names]\n",
    "    chars_flatten = list(itertools.chain(*chars))\n",
    "    freq = collections.Counter(chars_flatten)\n",
    "    freq = pd.DataFrame(freq.items(), columns=[\"char\", \"freq\"])\n",
    "    freq = freq.sort_values(by=\"freq\", ascending=False)\n",
    "    \n",
    "    # 准备数据\n",
    "    dict_size = 50\n",
    "    charset_size = dict_size + 1  # 加1是为了EOS token\n",
    "    dict = list(freq[\"char\"].values[:dict_size])\n",
    "    dict_set = set(dict)\n",
    "    dat = list(filter(lambda name: set(name).issubset(dict_set), names))\n",
    "    \n",
    "    return dat, dict, charset_size\n",
    "\n",
    "# 测试名字生成的数据准备函数\n",
    "print(\"测试名字生成的数据准备函数:\")\n",
    "try:\n",
    "    dat, dict, charset_size = prepare_name_data()\n",
    "    print(f\"数据大小: {len(dat)}\")\n",
    "    print(f\"字典大小: {len(dict)}\")\n",
    "    print(f\"字符集大小: {charset_size}\")\n",
    "    print(f\"前10个常用字: {dict[:10]}\")\n",
    "    print(f\"前5个名字示例: {dat[:5]}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"数据文件未找到，请确保文件路径正确\")\n",
    "    # 创建模拟数据用于测试\n",
    "    print(\"创建模拟数据用于测试...\")\n",
    "    np.random.seed(123)\n",
    "    mock_names = ['王伟', '李娜', '张强', '刘芳', '陈明', '赵美丽', '孙英俊'] * 100\n",
    "    dat = mock_names\n",
    "    dict = ['王', '李', '张', '刘', '陈', '赵', '孙', '伟', '娜', '强', '芳', '明', '美', '丽', '英', '俊']\n",
    "    charset_size = len(dict) + 1\n",
    "    \n",
    "    print(f\"模拟数据大小: {len(dat)}\")\n",
    "    print(f\"模拟字典大小: {len(dict)}\")\n",
    "    print(f\"模拟字符集大小: {charset_size}\")\n",
    "    print(f\"前5个名字示例: {dat[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf11a3a",
   "metadata": {},
   "source": [
    "## 名字生成的训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f78e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练名字生成器...\n",
      "Epoch 0, Batch 0, Loss = 4.1854\n",
      "Epoch 0, Batch 10, Loss = 3.7629\n",
      "Epoch 0, Batch 20, Loss = 3.6587\n",
      "Epoch 0, Batch 30, Loss = 3.6975\n",
      "Epoch 0, Batch 40, Loss = 3.6374\n",
      "Epoch 0, Batch 50, Loss = 3.5811\n",
      "Epoch 0, Batch 60, Loss = 3.6112\n",
      "Epoch 0, Batch 70, Loss = 3.5740\n",
      "Epoch 0, Batch 80, Loss = 3.5480\n",
      "Epoch 0, Batch 90, Loss = 3.5309\n",
      "Epoch 0, Batch 100, Loss = 3.5954\n",
      "Epoch 0, Batch 110, Loss = 3.4232\n",
      "Epoch 0, Batch 120, Loss = 3.4217\n",
      "Epoch 0, Batch 130, Loss = 3.5815\n",
      "Epoch 0, Batch 140, Loss = 3.3685\n",
      "Epoch 0, Batch 150, Loss = 3.4049\n",
      "Epoch 0, Batch 160, Loss = 3.3655\n",
      "Epoch 0, Batch 170, Loss = 3.2701\n",
      "Epoch 0, Batch 180, Loss = 3.3293\n",
      "Epoch 0, Batch 190, Loss = 3.3407\n",
      "Epoch 0, Batch 200, Loss = 3.3446\n",
      "Epoch 0, Batch 210, Loss = 3.3946\n",
      "Epoch 0, Batch 220, Loss = 3.5094\n",
      "Epoch 0, Batch 230, Loss = 3.5438\n",
      "Epoch 0, Batch 240, Loss = 3.4672\n",
      "Epoch 0, Batch 250, Loss = 3.4175\n",
      "Epoch 0, Batch 260, Loss = 3.4202\n",
      "Epoch 0, Batch 270, Loss = 3.4014\n",
      "Epoch 0, Batch 280, Loss = 3.2756\n",
      "Epoch 0, Batch 290, Loss = 3.4554\n",
      "Epoch 0, Batch 300, Loss = 3.3024\n",
      "Epoch 0, Batch 310, Loss = 3.4205\n",
      "Epoch 0, Batch 320, Loss = 3.3425\n",
      "Epoch 0, Batch 330, Loss = 3.2703\n",
      "Epoch 0, Batch 340, Loss = 3.4137\n",
      "Epoch 0, Batch 350, Loss = 3.4312\n",
      "Epoch 0, Batch 360, Loss = 3.1693\n",
      "Epoch 0, Batch 370, Loss = 3.2900\n",
      "Epoch 0, Batch 380, Loss = 3.2795\n",
      "Epoch 0, Batch 390, Loss = 3.2410\n",
      "Epoch 0, Batch 400, Loss = 3.4196\n",
      "Epoch 0, Batch 410, Loss = 3.2615\n",
      "Epoch 0, Batch 420, Loss = 3.3948\n",
      "Epoch 0, Batch 430, Loss = 3.2752\n",
      "Epoch 0, Batch 440, Loss = 3.3971\n",
      "Epoch 0, Batch 450, Loss = 3.3580\n",
      "Epoch 0, Batch 460, Loss = 3.3423\n",
      "Epoch 0, Batch 470, Loss = 3.3150\n",
      "Epoch 0, Batch 480, Loss = 3.2372\n",
      "Epoch 0, Batch 490, Loss = 3.3604\n",
      "Epoch 0, Batch 500, Loss = 3.3203\n",
      "Epoch 0, Batch 510, Loss = 3.3940\n",
      "Epoch 0, Batch 520, Loss = 3.3547\n",
      "Epoch 0, Batch 530, Loss = 3.3579\n",
      "Epoch 0, Batch 540, Loss = 3.2751\n",
      "Epoch 0, Batch 550, Loss = 3.3931\n",
      "Epoch 0, Batch 560, Loss = 3.3906\n",
      "Epoch 0, Batch 570, Loss = 3.4073\n",
      "Epoch 0, Batch 580, Loss = 3.4662\n",
      "Epoch 0, Batch 590, Loss = 3.1511\n",
      "Epoch 0, Batch 600, Loss = 3.2903\n",
      "Epoch 0, Batch 610, Loss = 3.4702\n",
      "Epoch 0, Batch 620, Loss = 3.2274\n",
      "Epoch 0, Batch 630, Loss = 3.2068\n",
      "Epoch 0, Batch 640, Loss = 3.2876\n",
      "Epoch 0, Batch 650, Loss = 3.2537\n",
      "Epoch 0, Batch 660, Loss = 3.2723\n",
      "Epoch 0, Batch 670, Loss = 3.2265\n",
      "Epoch 0, Batch 680, Loss = 3.3747\n",
      "Epoch 0, Batch 690, Loss = 3.3802\n",
      "Epoch 0, Batch 700, Loss = 3.3216\n",
      "Epoch 0, Batch 710, Loss = 3.3293\n",
      "Epoch 0, Batch 720, Loss = 3.2296\n",
      "Epoch 0, Batch 730, Loss = 3.3956\n",
      "Epoch 0, Batch 740, Loss = 3.3353\n",
      "Epoch 0, Batch 750, Loss = 3.3909\n",
      "Epoch 0, Batch 760, Loss = 3.5269\n",
      "Epoch 0, Batch 770, Loss = 3.3115\n",
      "Epoch 0, Batch 780, Loss = 3.3922\n",
      "Epoch 0, Batch 790, Loss = 3.4828\n",
      "Epoch 0, Batch 800, Loss = 3.3397\n",
      "Epoch 0, Batch 810, Loss = 3.3867\n",
      "Epoch 0, Batch 820, Loss = 3.2565\n",
      "Epoch 0, Batch 830, Loss = 3.1582\n",
      "Epoch 0, Batch 840, Loss = 3.3906\n",
      "Epoch 0, Batch 850, Loss = 3.3555\n",
      "Epoch 0, Batch 860, Loss = 3.3935\n",
      "Epoch 0, Batch 870, Loss = 3.4151\n",
      "Epoch 0, Batch 880, Loss = 3.2724\n",
      "Epoch 0, Batch 890, Loss = 3.1755\n",
      "Epoch 0, Batch 900, Loss = 3.2299\n",
      "Epoch 0, Batch 910, Loss = 3.2997\n",
      "Epoch 0, Batch 920, Loss = 3.4625\n",
      "Epoch 0, Batch 930, Loss = 3.3570\n",
      "Epoch 0, Batch 940, Loss = 3.2944\n",
      "Epoch 0, Batch 950, Loss = 3.2678\n",
      "Epoch 0, Batch 960, Loss = 3.4133\n",
      "Epoch 0, Batch 970, Loss = 3.4406\n",
      "Epoch 0, Batch 980, Loss = 3.2940\n",
      "Epoch 0, Batch 990, Loss = 3.4603\n",
      "Epoch 0, Batch 1000, Loss = 3.3657\n",
      "Epoch 0, Batch 1010, Loss = 3.3071\n",
      "Epoch 0, Batch 1020, Loss = 3.2995\n",
      "Epoch 0, Batch 1030, Loss = 3.3312\n",
      "Epoch 0, Batch 1040, Loss = 3.3232\n",
      "Epoch 0, Batch 1050, Loss = 3.1978\n",
      "Epoch 0, Batch 1060, Loss = 3.2942\n",
      "Epoch 0, Batch 1070, Loss = 3.3359\n",
      "Epoch 0, Batch 1080, Loss = 3.3264\n",
      "Epoch 0, Batch 1090, Loss = 3.2839\n",
      "Epoch 0, Batch 1100, Loss = 3.3476\n",
      "Epoch 0, Batch 1110, Loss = 3.1222\n",
      "Epoch 0, Batch 1120, Loss = 3.3742\n",
      "Epoch 0, Average Loss = 3.3695\n",
      "Epoch 1, Batch 0, Loss = 3.2021\n",
      "Epoch 1, Batch 10, Loss = 3.3870\n",
      "Epoch 1, Batch 20, Loss = 3.3579\n",
      "Epoch 1, Batch 30, Loss = 3.2348\n",
      "Epoch 1, Batch 40, Loss = 3.2301\n",
      "Epoch 1, Batch 50, Loss = 3.3679\n",
      "Epoch 1, Batch 60, Loss = 3.3252\n",
      "Epoch 1, Batch 70, Loss = 3.3530\n",
      "Epoch 1, Batch 80, Loss = 3.1902\n",
      "Epoch 1, Batch 90, Loss = 3.3387\n",
      "Epoch 1, Batch 100, Loss = 3.2490\n",
      "Epoch 1, Batch 110, Loss = 3.3112\n",
      "Epoch 1, Batch 120, Loss = 3.2970\n",
      "Epoch 1, Batch 130, Loss = 3.3675\n",
      "Epoch 1, Batch 140, Loss = 3.1864\n",
      "Epoch 1, Batch 150, Loss = 3.4509\n",
      "Epoch 1, Batch 160, Loss = 3.1998\n",
      "Epoch 1, Batch 170, Loss = 3.2266\n",
      "Epoch 1, Batch 180, Loss = 3.2067\n",
      "Epoch 1, Batch 190, Loss = 3.3234\n",
      "Epoch 1, Batch 200, Loss = 3.1855\n",
      "Epoch 1, Batch 210, Loss = 3.3706\n",
      "Epoch 1, Batch 220, Loss = 3.4533\n",
      "Epoch 1, Batch 230, Loss = 3.4114\n",
      "Epoch 1, Batch 240, Loss = 3.2339\n",
      "Epoch 1, Batch 250, Loss = 3.2625\n",
      "Epoch 1, Batch 260, Loss = 3.1812\n",
      "Epoch 1, Batch 270, Loss = 3.4186\n",
      "Epoch 1, Batch 280, Loss = 3.2293\n",
      "Epoch 1, Batch 290, Loss = 3.2569\n",
      "Epoch 1, Batch 300, Loss = 3.2646\n",
      "Epoch 1, Batch 310, Loss = 3.3237\n",
      "Epoch 1, Batch 320, Loss = 3.3270\n",
      "Epoch 1, Batch 330, Loss = 3.3024\n",
      "Epoch 1, Batch 340, Loss = 3.2822\n",
      "Epoch 1, Batch 350, Loss = 3.2553\n",
      "Epoch 1, Batch 360, Loss = 3.1575\n",
      "Epoch 1, Batch 370, Loss = 3.1512\n",
      "Epoch 1, Batch 380, Loss = 3.2489\n",
      "Epoch 1, Batch 390, Loss = 3.3402\n",
      "Epoch 1, Batch 400, Loss = 3.3465\n",
      "Epoch 1, Batch 410, Loss = 3.3102\n",
      "Epoch 1, Batch 420, Loss = 3.2839\n",
      "Epoch 1, Batch 430, Loss = 3.1331\n",
      "Epoch 1, Batch 440, Loss = 3.2372\n",
      "Epoch 1, Batch 450, Loss = 3.2978\n",
      "Epoch 1, Batch 460, Loss = 3.2903\n",
      "Epoch 1, Batch 470, Loss = 3.2852\n",
      "Epoch 1, Batch 480, Loss = 3.2574\n",
      "Epoch 1, Batch 490, Loss = 3.3560\n",
      "Epoch 1, Batch 500, Loss = 3.2737\n",
      "Epoch 1, Batch 510, Loss = 3.4860\n",
      "Epoch 1, Batch 520, Loss = 3.3717\n",
      "Epoch 1, Batch 530, Loss = 3.3532\n",
      "Epoch 1, Batch 540, Loss = 3.2462\n",
      "Epoch 1, Batch 550, Loss = 3.4343\n",
      "Epoch 1, Batch 560, Loss = 3.3512\n",
      "Epoch 1, Batch 570, Loss = 3.1444\n",
      "Epoch 1, Batch 580, Loss = 3.2044\n",
      "Epoch 1, Batch 590, Loss = 3.2755\n",
      "Epoch 1, Batch 600, Loss = 3.2706\n",
      "Epoch 1, Batch 610, Loss = 3.3279\n",
      "Epoch 1, Batch 620, Loss = 3.1299\n",
      "Epoch 1, Batch 630, Loss = 3.2167\n",
      "Epoch 1, Batch 640, Loss = 3.1429\n",
      "Epoch 1, Batch 650, Loss = 3.2643\n",
      "Epoch 1, Batch 660, Loss = 3.3239\n",
      "Epoch 1, Batch 670, Loss = 3.3123\n",
      "Epoch 1, Batch 680, Loss = 3.2172\n",
      "Epoch 1, Batch 690, Loss = 3.4467\n",
      "Epoch 1, Batch 700, Loss = 3.3670\n",
      "Epoch 1, Batch 710, Loss = 3.3332\n",
      "Epoch 1, Batch 720, Loss = 3.1940\n",
      "Epoch 1, Batch 730, Loss = 3.2729\n",
      "Epoch 1, Batch 740, Loss = 3.1269\n",
      "Epoch 1, Batch 750, Loss = 3.2202\n",
      "Epoch 1, Batch 760, Loss = 3.2410\n",
      "Epoch 1, Batch 770, Loss = 3.3018\n",
      "Epoch 1, Batch 780, Loss = 3.3270\n",
      "Epoch 1, Batch 790, Loss = 3.2848\n",
      "Epoch 1, Batch 800, Loss = 3.0403\n",
      "Epoch 1, Batch 810, Loss = 3.3296\n",
      "Epoch 1, Batch 820, Loss = 3.3578\n",
      "Epoch 1, Batch 830, Loss = 3.1565\n",
      "Epoch 1, Batch 840, Loss = 3.2598\n",
      "Epoch 1, Batch 850, Loss = 3.2641\n",
      "Epoch 1, Batch 860, Loss = 3.3611\n",
      "Epoch 1, Batch 870, Loss = 3.3843\n",
      "Epoch 1, Batch 880, Loss = 3.2453\n",
      "Epoch 1, Batch 890, Loss = 3.3430\n",
      "Epoch 1, Batch 900, Loss = 3.1872\n",
      "Epoch 1, Batch 910, Loss = 3.2374\n",
      "Epoch 1, Batch 920, Loss = 3.3066\n",
      "Epoch 1, Batch 930, Loss = 3.2813\n",
      "Epoch 1, Batch 940, Loss = 3.3264\n",
      "Epoch 1, Batch 950, Loss = 3.2357\n",
      "Epoch 1, Batch 960, Loss = 3.4239\n",
      "Epoch 1, Batch 970, Loss = 3.2453\n",
      "Epoch 1, Batch 980, Loss = 3.3438\n",
      "Epoch 1, Batch 990, Loss = 3.4300\n",
      "Epoch 1, Batch 1000, Loss = 3.1565\n",
      "Epoch 1, Batch 1010, Loss = 3.1851\n",
      "Epoch 1, Batch 1020, Loss = 3.0765\n",
      "Epoch 1, Batch 1030, Loss = 3.1486\n",
      "Epoch 1, Batch 1040, Loss = 3.3691\n",
      "Epoch 1, Batch 1050, Loss = 3.3791\n",
      "Epoch 1, Batch 1060, Loss = 3.2215\n",
      "Epoch 1, Batch 1070, Loss = 3.3348\n",
      "Epoch 1, Batch 1080, Loss = 3.2971\n",
      "Epoch 1, Batch 1090, Loss = 3.3354\n",
      "Epoch 1, Batch 1100, Loss = 3.2180\n",
      "Epoch 1, Batch 1110, Loss = 3.1939\n",
      "Epoch 1, Batch 1120, Loss = 3.1931\n",
      "Epoch 1, Average Loss = 3.2741\n",
      "Epoch 2, Batch 0, Loss = 3.2109\n",
      "Epoch 2, Batch 10, Loss = 3.3480\n",
      "Epoch 2, Batch 20, Loss = 3.4615\n",
      "Epoch 2, Batch 30, Loss = 3.2643\n",
      "Epoch 2, Batch 40, Loss = 3.3695\n",
      "Epoch 2, Batch 50, Loss = 3.1380\n",
      "Epoch 2, Batch 60, Loss = 3.3458\n",
      "Epoch 2, Batch 70, Loss = 3.4563\n",
      "Epoch 2, Batch 80, Loss = 3.2841\n",
      "Epoch 2, Batch 90, Loss = 3.2196\n",
      "Epoch 2, Batch 100, Loss = 3.0413\n",
      "Epoch 2, Batch 110, Loss = 3.2892\n",
      "Epoch 2, Batch 120, Loss = 3.2075\n",
      "Epoch 2, Batch 130, Loss = 3.2519\n",
      "Epoch 2, Batch 140, Loss = 3.3682\n",
      "Epoch 2, Batch 150, Loss = 3.3221\n",
      "Epoch 2, Batch 160, Loss = 3.1057\n",
      "Epoch 2, Batch 170, Loss = 3.1113\n",
      "Epoch 2, Batch 180, Loss = 3.2576\n",
      "Epoch 2, Batch 190, Loss = 3.1454\n",
      "Epoch 2, Batch 200, Loss = 3.2798\n",
      "Epoch 2, Batch 210, Loss = 3.1967\n",
      "Epoch 2, Batch 220, Loss = 3.3172\n",
      "Epoch 2, Batch 230, Loss = 3.0767\n",
      "Epoch 2, Batch 240, Loss = 3.3680\n",
      "Epoch 2, Batch 250, Loss = 3.3292\n",
      "Epoch 2, Batch 260, Loss = 3.2925\n",
      "Epoch 2, Batch 270, Loss = 3.2410\n",
      "Epoch 2, Batch 280, Loss = 3.2170\n",
      "Epoch 2, Batch 290, Loss = 3.2375\n",
      "Epoch 2, Batch 300, Loss = 3.3039\n",
      "Epoch 2, Batch 310, Loss = 3.1356\n",
      "Epoch 2, Batch 320, Loss = 3.3065\n",
      "Epoch 2, Batch 330, Loss = 3.2999\n",
      "Epoch 2, Batch 340, Loss = 3.3031\n",
      "Epoch 2, Batch 350, Loss = 3.3863\n",
      "Epoch 2, Batch 360, Loss = 3.1722\n",
      "Epoch 2, Batch 370, Loss = 3.2358\n",
      "Epoch 2, Batch 380, Loss = 3.2421\n",
      "Epoch 2, Batch 390, Loss = 3.3883\n",
      "Epoch 2, Batch 400, Loss = 2.9725\n",
      "Epoch 2, Batch 410, Loss = 3.2972\n",
      "Epoch 2, Batch 420, Loss = 3.2510\n",
      "Epoch 2, Batch 430, Loss = 3.3360\n",
      "Epoch 2, Batch 440, Loss = 3.1811\n",
      "Epoch 2, Batch 450, Loss = 3.3090\n",
      "Epoch 2, Batch 460, Loss = 3.2776\n",
      "Epoch 2, Batch 470, Loss = 3.3164\n",
      "Epoch 2, Batch 480, Loss = 3.3427\n",
      "Epoch 2, Batch 490, Loss = 3.2914\n",
      "Epoch 2, Batch 500, Loss = 3.3133\n",
      "Epoch 2, Batch 510, Loss = 3.2344\n",
      "Epoch 2, Batch 520, Loss = 3.4067\n",
      "Epoch 2, Batch 530, Loss = 3.3852\n",
      "Epoch 2, Batch 540, Loss = 3.2124\n",
      "Epoch 2, Batch 550, Loss = 3.2762\n",
      "Epoch 2, Batch 560, Loss = 3.2186\n",
      "Epoch 2, Batch 570, Loss = 3.3883\n",
      "Epoch 2, Batch 580, Loss = 3.2221\n",
      "Epoch 2, Batch 590, Loss = 3.2787\n",
      "Epoch 2, Batch 600, Loss = 3.2586\n",
      "Epoch 2, Batch 610, Loss = 3.3072\n",
      "Epoch 2, Batch 620, Loss = 3.3815\n",
      "Epoch 2, Batch 630, Loss = 3.2078\n",
      "Epoch 2, Batch 640, Loss = 3.2609\n",
      "Epoch 2, Batch 650, Loss = 3.3375\n",
      "Epoch 2, Batch 660, Loss = 3.2873\n",
      "Epoch 2, Batch 670, Loss = 3.3970\n",
      "Epoch 2, Batch 680, Loss = 3.3339\n",
      "Epoch 2, Batch 690, Loss = 3.2374\n",
      "Epoch 2, Batch 700, Loss = 3.3858\n",
      "Epoch 2, Batch 710, Loss = 3.1739\n",
      "Epoch 2, Batch 720, Loss = 3.2434\n",
      "Epoch 2, Batch 730, Loss = 3.2580\n",
      "Epoch 2, Batch 740, Loss = 3.1405\n",
      "Epoch 2, Batch 750, Loss = 3.3640\n",
      "Epoch 2, Batch 760, Loss = 3.1116\n",
      "Epoch 2, Batch 770, Loss = 3.0950\n",
      "Epoch 2, Batch 780, Loss = 3.3248\n",
      "Epoch 2, Batch 790, Loss = 3.1827\n",
      "Epoch 2, Batch 800, Loss = 3.1802\n",
      "Epoch 2, Batch 810, Loss = 3.0504\n",
      "Epoch 2, Batch 820, Loss = 3.1657\n",
      "Epoch 2, Batch 830, Loss = 3.2652\n",
      "Epoch 2, Batch 840, Loss = 3.1548\n",
      "Epoch 2, Batch 850, Loss = 3.1700\n",
      "Epoch 2, Batch 860, Loss = 3.3746\n",
      "Epoch 2, Batch 870, Loss = 3.2451\n",
      "Epoch 2, Batch 880, Loss = 3.1147\n",
      "Epoch 2, Batch 890, Loss = 3.2079\n",
      "Epoch 2, Batch 900, Loss = 3.3054\n",
      "Epoch 2, Batch 910, Loss = 3.0950\n",
      "Epoch 2, Batch 920, Loss = 3.2220\n",
      "Epoch 2, Batch 930, Loss = 3.1430\n",
      "Epoch 2, Batch 940, Loss = 3.2663\n",
      "Epoch 2, Batch 950, Loss = 3.0687\n",
      "Epoch 2, Batch 960, Loss = 3.2431\n",
      "Epoch 2, Batch 970, Loss = 3.3139\n",
      "Epoch 2, Batch 980, Loss = 3.3007\n",
      "Epoch 2, Batch 990, Loss = 3.2869\n",
      "Epoch 2, Batch 1000, Loss = 3.4776\n",
      "Epoch 2, Batch 1010, Loss = 3.2893\n",
      "Epoch 2, Batch 1020, Loss = 3.1969\n",
      "Epoch 2, Batch 1030, Loss = 3.0687\n",
      "Epoch 2, Batch 1040, Loss = 3.1541\n",
      "Epoch 2, Batch 1050, Loss = 3.1932\n",
      "Epoch 2, Batch 1060, Loss = 3.3841\n",
      "Epoch 2, Batch 1070, Loss = 3.3639\n",
      "Epoch 2, Batch 1080, Loss = 3.2401\n",
      "Epoch 2, Batch 1090, Loss = 3.4773\n",
      "Epoch 2, Batch 1100, Loss = 3.2490\n",
      "Epoch 2, Batch 1110, Loss = 3.2385\n",
      "Epoch 2, Batch 1120, Loss = 3.2207\n",
      "Epoch 2, Average Loss = 3.2485\n",
      "Epoch 3, Batch 0, Loss = 3.2663\n",
      "Epoch 3, Batch 10, Loss = 3.2406\n",
      "Epoch 3, Batch 20, Loss = 3.2380\n",
      "Epoch 3, Batch 30, Loss = 3.2042\n",
      "Epoch 3, Batch 40, Loss = 3.2455\n",
      "Epoch 3, Batch 50, Loss = 3.2026\n",
      "Epoch 3, Batch 60, Loss = 3.3051\n",
      "Epoch 3, Batch 70, Loss = 3.1963\n",
      "Epoch 3, Batch 80, Loss = 3.3448\n",
      "Epoch 3, Batch 90, Loss = 3.2791\n",
      "Epoch 3, Batch 100, Loss = 3.0360\n",
      "Epoch 3, Batch 110, Loss = 3.1936\n",
      "Epoch 3, Batch 120, Loss = 3.2710\n",
      "Epoch 3, Batch 130, Loss = 3.2610\n",
      "Epoch 3, Batch 140, Loss = 3.2506\n",
      "Epoch 3, Batch 150, Loss = 3.2322\n",
      "Epoch 3, Batch 160, Loss = 3.1729\n",
      "Epoch 3, Batch 170, Loss = 3.2294\n",
      "Epoch 3, Batch 180, Loss = 3.2579\n",
      "Epoch 3, Batch 190, Loss = 3.2075\n",
      "Epoch 3, Batch 200, Loss = 3.2414\n",
      "Epoch 3, Batch 210, Loss = 3.2247\n",
      "Epoch 3, Batch 220, Loss = 3.2765\n",
      "Epoch 3, Batch 230, Loss = 3.2159\n",
      "Epoch 3, Batch 240, Loss = 3.2693\n",
      "Epoch 3, Batch 250, Loss = 3.3139\n",
      "Epoch 3, Batch 260, Loss = 3.1246\n",
      "Epoch 3, Batch 270, Loss = 3.1692\n",
      "Epoch 3, Batch 280, Loss = 3.3707\n",
      "Epoch 3, Batch 290, Loss = 3.2070\n",
      "Epoch 3, Batch 300, Loss = 3.1401\n",
      "Epoch 3, Batch 310, Loss = 3.0741\n",
      "Epoch 3, Batch 320, Loss = 3.2071\n",
      "Epoch 3, Batch 330, Loss = 3.2464\n",
      "Epoch 3, Batch 340, Loss = 3.2239\n",
      "Epoch 3, Batch 350, Loss = 3.1667\n",
      "Epoch 3, Batch 360, Loss = 3.2127\n",
      "Epoch 3, Batch 370, Loss = 3.3679\n",
      "Epoch 3, Batch 380, Loss = 3.2378\n",
      "Epoch 3, Batch 390, Loss = 3.2748\n",
      "Epoch 3, Batch 400, Loss = 3.3263\n",
      "Epoch 3, Batch 410, Loss = 3.3309\n",
      "Epoch 3, Batch 420, Loss = 3.2067\n",
      "Epoch 3, Batch 430, Loss = 3.3231\n",
      "Epoch 3, Batch 440, Loss = 3.1475\n",
      "Epoch 3, Batch 450, Loss = 3.1410\n",
      "Epoch 3, Batch 460, Loss = 3.2987\n",
      "Epoch 3, Batch 470, Loss = 3.2584\n",
      "Epoch 3, Batch 480, Loss = 3.0856\n",
      "Epoch 3, Batch 490, Loss = 3.0707\n",
      "Epoch 3, Batch 500, Loss = 3.1216\n",
      "Epoch 3, Batch 510, Loss = 3.2595\n",
      "Epoch 3, Batch 520, Loss = 3.1835\n",
      "Epoch 3, Batch 530, Loss = 3.2709\n",
      "Epoch 3, Batch 540, Loss = 3.2092\n",
      "Epoch 3, Batch 550, Loss = 3.2421\n",
      "Epoch 3, Batch 560, Loss = 3.1658\n",
      "Epoch 3, Batch 570, Loss = 3.1259\n",
      "Epoch 3, Batch 580, Loss = 3.1178\n",
      "Epoch 3, Batch 590, Loss = 3.1857\n",
      "Epoch 3, Batch 600, Loss = 3.1278\n",
      "Epoch 3, Batch 610, Loss = 3.2466\n",
      "Epoch 3, Batch 620, Loss = 3.2144\n",
      "Epoch 3, Batch 630, Loss = 3.0923\n",
      "Epoch 3, Batch 640, Loss = 3.2054\n",
      "Epoch 3, Batch 650, Loss = 3.2477\n",
      "Epoch 3, Batch 660, Loss = 3.2969\n",
      "Epoch 3, Batch 670, Loss = 3.2490\n",
      "Epoch 3, Batch 680, Loss = 3.3047\n",
      "Epoch 3, Batch 690, Loss = 3.2062\n",
      "Epoch 3, Batch 700, Loss = 3.4064\n",
      "Epoch 3, Batch 710, Loss = 3.3017\n",
      "Epoch 3, Batch 720, Loss = 3.2002\n",
      "Epoch 3, Batch 730, Loss = 3.1356\n",
      "Epoch 3, Batch 740, Loss = 3.2345\n",
      "Epoch 3, Batch 750, Loss = 3.1954\n",
      "Epoch 3, Batch 760, Loss = 3.2131\n",
      "Epoch 3, Batch 770, Loss = 3.3480\n",
      "Epoch 3, Batch 780, Loss = 3.1847\n",
      "Epoch 3, Batch 790, Loss = 3.2725\n",
      "Epoch 3, Batch 800, Loss = 3.3065\n",
      "Epoch 3, Batch 810, Loss = 3.2449\n",
      "Epoch 3, Batch 820, Loss = 3.3033\n",
      "Epoch 3, Batch 830, Loss = 3.3154\n",
      "Epoch 3, Batch 840, Loss = 3.5686\n",
      "Epoch 3, Batch 850, Loss = 3.0795\n",
      "Epoch 3, Batch 860, Loss = 3.2006\n",
      "Epoch 3, Batch 870, Loss = 3.2331\n",
      "Epoch 3, Batch 880, Loss = 3.2765\n",
      "Epoch 3, Batch 890, Loss = 3.2712\n",
      "Epoch 3, Batch 900, Loss = 3.3062\n",
      "Epoch 3, Batch 910, Loss = 3.1887\n",
      "Epoch 3, Batch 920, Loss = 3.2136\n",
      "Epoch 3, Batch 930, Loss = 3.2600\n",
      "Epoch 3, Batch 940, Loss = 3.2098\n",
      "Epoch 3, Batch 950, Loss = 3.2307\n",
      "Epoch 3, Batch 960, Loss = 3.1418\n",
      "Epoch 3, Batch 970, Loss = 3.1477\n",
      "Epoch 3, Batch 980, Loss = 3.1509\n",
      "Epoch 3, Batch 990, Loss = 3.2756\n",
      "Epoch 3, Batch 1000, Loss = 3.2444\n",
      "Epoch 3, Batch 1010, Loss = 3.2336\n",
      "Epoch 3, Batch 1020, Loss = 3.2424\n",
      "Epoch 3, Batch 1030, Loss = 3.1350\n",
      "Epoch 3, Batch 1040, Loss = 3.2465\n",
      "Epoch 3, Batch 1050, Loss = 3.1764\n",
      "Epoch 3, Batch 1060, Loss = 3.2863\n",
      "Epoch 3, Batch 1070, Loss = 3.2651\n",
      "Epoch 3, Batch 1080, Loss = 3.2455\n",
      "Epoch 3, Batch 1090, Loss = 3.1563\n",
      "Epoch 3, Batch 1100, Loss = 3.1536\n",
      "Epoch 3, Batch 1110, Loss = 3.3364\n",
      "Epoch 3, Batch 1120, Loss = 3.1482\n",
      "Epoch 3, Average Loss = 3.2323\n",
      "Epoch 4, Batch 0, Loss = 3.2168\n",
      "Epoch 4, Batch 10, Loss = 3.2330\n",
      "Epoch 4, Batch 20, Loss = 3.1945\n",
      "Epoch 4, Batch 30, Loss = 3.1843\n",
      "Epoch 4, Batch 40, Loss = 3.2304\n",
      "Epoch 4, Batch 50, Loss = 3.1879\n",
      "Epoch 4, Batch 60, Loss = 3.0315\n",
      "Epoch 4, Batch 70, Loss = 3.2869\n",
      "Epoch 4, Batch 80, Loss = 3.1302\n",
      "Epoch 4, Batch 90, Loss = 3.4070\n",
      "Epoch 4, Batch 100, Loss = 3.1985\n",
      "Epoch 4, Batch 110, Loss = 3.1576\n",
      "Epoch 4, Batch 120, Loss = 3.2132\n",
      "Epoch 4, Batch 130, Loss = 3.2614\n",
      "Epoch 4, Batch 140, Loss = 3.1455\n",
      "Epoch 4, Batch 150, Loss = 3.1170\n",
      "Epoch 4, Batch 160, Loss = 3.0589\n",
      "Epoch 4, Batch 170, Loss = 3.2783\n",
      "Epoch 4, Batch 180, Loss = 3.1787\n",
      "Epoch 4, Batch 190, Loss = 3.1466\n",
      "Epoch 4, Batch 200, Loss = 3.2371\n",
      "Epoch 4, Batch 210, Loss = 3.0527\n",
      "Epoch 4, Batch 220, Loss = 3.3459\n",
      "Epoch 4, Batch 230, Loss = 3.1353\n",
      "Epoch 4, Batch 240, Loss = 3.1531\n",
      "Epoch 4, Batch 250, Loss = 3.2580\n",
      "Epoch 4, Batch 260, Loss = 3.1870\n",
      "Epoch 4, Batch 270, Loss = 3.2554\n",
      "Epoch 4, Batch 280, Loss = 3.2079\n",
      "Epoch 4, Batch 290, Loss = 3.0954\n",
      "Epoch 4, Batch 300, Loss = 3.0719\n",
      "Epoch 4, Batch 310, Loss = 3.1175\n",
      "Epoch 4, Batch 320, Loss = 3.2744\n",
      "Epoch 4, Batch 330, Loss = 3.1239\n",
      "Epoch 4, Batch 340, Loss = 3.3481\n",
      "Epoch 4, Batch 350, Loss = 3.0544\n",
      "Epoch 4, Batch 360, Loss = 3.1819\n",
      "Epoch 4, Batch 370, Loss = 3.2434\n",
      "Epoch 4, Batch 380, Loss = 3.1968\n",
      "Epoch 4, Batch 390, Loss = 3.0300\n",
      "Epoch 4, Batch 400, Loss = 3.3693\n",
      "Epoch 4, Batch 410, Loss = 3.2048\n",
      "Epoch 4, Batch 420, Loss = 3.2306\n",
      "Epoch 4, Batch 430, Loss = 3.3193\n",
      "Epoch 4, Batch 440, Loss = 3.2386\n",
      "Epoch 4, Batch 450, Loss = 3.1148\n",
      "Epoch 4, Batch 460, Loss = 3.1910\n",
      "Epoch 4, Batch 470, Loss = 3.3127\n",
      "Epoch 4, Batch 480, Loss = 3.3316\n",
      "Epoch 4, Batch 490, Loss = 3.1871\n",
      "Epoch 4, Batch 500, Loss = 3.3024\n",
      "Epoch 4, Batch 510, Loss = 3.1432\n",
      "Epoch 4, Batch 520, Loss = 3.4560\n",
      "Epoch 4, Batch 530, Loss = 3.2071\n",
      "Epoch 4, Batch 540, Loss = 3.2783\n",
      "Epoch 4, Batch 550, Loss = 3.2554\n",
      "Epoch 4, Batch 560, Loss = 3.3290\n",
      "Epoch 4, Batch 570, Loss = 3.3257\n",
      "Epoch 4, Batch 580, Loss = 3.1976\n",
      "Epoch 4, Batch 590, Loss = 3.3252\n",
      "Epoch 4, Batch 600, Loss = 3.2589\n",
      "Epoch 4, Batch 610, Loss = 3.2770\n",
      "Epoch 4, Batch 620, Loss = 3.1583\n",
      "Epoch 4, Batch 630, Loss = 3.2055\n",
      "Epoch 4, Batch 640, Loss = 3.1163\n",
      "Epoch 4, Batch 650, Loss = 3.0140\n",
      "Epoch 4, Batch 660, Loss = 3.3402\n",
      "Epoch 4, Batch 670, Loss = 3.2236\n",
      "Epoch 4, Batch 680, Loss = 3.3012\n",
      "Epoch 4, Batch 690, Loss = 3.2784\n",
      "Epoch 4, Batch 700, Loss = 3.2713\n",
      "Epoch 4, Batch 710, Loss = 3.1228\n",
      "Epoch 4, Batch 720, Loss = 3.1380\n",
      "Epoch 4, Batch 730, Loss = 3.3129\n",
      "Epoch 4, Batch 740, Loss = 3.3138\n",
      "Epoch 4, Batch 750, Loss = 3.2880\n",
      "Epoch 4, Batch 760, Loss = 3.2337\n",
      "Epoch 4, Batch 770, Loss = 3.2168\n",
      "Epoch 4, Batch 780, Loss = 3.0502\n",
      "Epoch 4, Batch 790, Loss = 3.2940\n",
      "Epoch 4, Batch 800, Loss = 3.1345\n",
      "Epoch 4, Batch 810, Loss = 3.1594\n",
      "Epoch 4, Batch 820, Loss = 3.3078\n",
      "Epoch 4, Batch 830, Loss = 3.2477\n",
      "Epoch 4, Batch 840, Loss = 3.1633\n",
      "Epoch 4, Batch 850, Loss = 3.1410\n",
      "Epoch 4, Batch 860, Loss = 3.2189\n",
      "Epoch 4, Batch 870, Loss = 3.1916\n",
      "Epoch 4, Batch 880, Loss = 3.2823\n",
      "Epoch 4, Batch 890, Loss = 3.2194\n",
      "Epoch 4, Batch 900, Loss = 3.1919\n",
      "Epoch 4, Batch 910, Loss = 3.2505\n",
      "Epoch 4, Batch 920, Loss = 3.2686\n",
      "Epoch 4, Batch 930, Loss = 3.1536\n",
      "Epoch 4, Batch 940, Loss = 3.3359\n",
      "Epoch 4, Batch 950, Loss = 3.2960\n",
      "Epoch 4, Batch 960, Loss = 3.0933\n",
      "Epoch 4, Batch 970, Loss = 3.1186\n",
      "Epoch 4, Batch 980, Loss = 3.3170\n",
      "Epoch 4, Batch 990, Loss = 3.2325\n",
      "Epoch 4, Batch 1000, Loss = 3.1503\n",
      "Epoch 4, Batch 1010, Loss = 3.2357\n",
      "Epoch 4, Batch 1020, Loss = 3.1958\n",
      "Epoch 4, Batch 1030, Loss = 3.2374\n",
      "Epoch 4, Batch 1040, Loss = 3.2742\n",
      "Epoch 4, Batch 1050, Loss = 3.2610\n",
      "Epoch 4, Batch 1060, Loss = 3.2276\n",
      "Epoch 4, Batch 1070, Loss = 3.2703\n",
      "Epoch 4, Batch 1080, Loss = 3.2277\n",
      "Epoch 4, Batch 1090, Loss = 3.2191\n",
      "Epoch 4, Batch 1100, Loss = 3.1829\n",
      "Epoch 4, Batch 1110, Loss = 3.0921\n",
      "Epoch 4, Batch 1120, Loss = 3.0260\n",
      "Epoch 4, Average Loss = 3.2190\n",
      "Epoch 5, Batch 0, Loss = 3.2002\n",
      "Epoch 5, Batch 10, Loss = 3.1230\n",
      "Epoch 5, Batch 20, Loss = 3.1953\n",
      "Epoch 5, Batch 30, Loss = 3.2858\n",
      "Epoch 5, Batch 40, Loss = 3.0902\n",
      "Epoch 5, Batch 50, Loss = 3.2357\n",
      "Epoch 5, Batch 60, Loss = 3.2867\n",
      "Epoch 5, Batch 70, Loss = 3.2262\n",
      "Epoch 5, Batch 80, Loss = 3.1206\n",
      "Epoch 5, Batch 90, Loss = 3.2939\n",
      "Epoch 5, Batch 100, Loss = 3.1925\n",
      "Epoch 5, Batch 110, Loss = 3.1552\n",
      "Epoch 5, Batch 120, Loss = 3.3598\n",
      "Epoch 5, Batch 130, Loss = 3.2972\n",
      "Epoch 5, Batch 140, Loss = 3.1202\n",
      "Epoch 5, Batch 150, Loss = 3.3360\n",
      "Epoch 5, Batch 160, Loss = 3.1881\n",
      "Epoch 5, Batch 170, Loss = 3.3093\n",
      "Epoch 5, Batch 180, Loss = 3.2999\n",
      "Epoch 5, Batch 190, Loss = 3.1897\n",
      "Epoch 5, Batch 200, Loss = 3.2289\n",
      "Epoch 5, Batch 210, Loss = 3.0666\n",
      "Epoch 5, Batch 220, Loss = 3.3676\n",
      "Epoch 5, Batch 230, Loss = 3.2533\n",
      "Epoch 5, Batch 240, Loss = 3.1492\n",
      "Epoch 5, Batch 250, Loss = 3.1931\n",
      "Epoch 5, Batch 260, Loss = 3.2026\n",
      "Epoch 5, Batch 270, Loss = 3.1076\n",
      "Epoch 5, Batch 280, Loss = 3.1147\n",
      "Epoch 5, Batch 290, Loss = 3.0953\n",
      "Epoch 5, Batch 300, Loss = 3.1664\n",
      "Epoch 5, Batch 310, Loss = 3.2464\n",
      "Epoch 5, Batch 320, Loss = 3.2407\n",
      "Epoch 5, Batch 330, Loss = 3.1531\n",
      "Epoch 5, Batch 340, Loss = 3.3293\n",
      "Epoch 5, Batch 350, Loss = 3.1043\n",
      "Epoch 5, Batch 360, Loss = 3.2562\n",
      "Epoch 5, Batch 370, Loss = 3.3381\n",
      "Epoch 5, Batch 380, Loss = 3.0606\n",
      "Epoch 5, Batch 390, Loss = 3.0609\n",
      "Epoch 5, Batch 400, Loss = 3.1698\n",
      "Epoch 5, Batch 410, Loss = 3.3028\n",
      "Epoch 5, Batch 420, Loss = 3.0989\n",
      "Epoch 5, Batch 430, Loss = 3.0401\n",
      "Epoch 5, Batch 440, Loss = 3.3408\n",
      "Epoch 5, Batch 450, Loss = 3.3627\n",
      "Epoch 5, Batch 460, Loss = 3.2560\n",
      "Epoch 5, Batch 470, Loss = 3.1077\n",
      "Epoch 5, Batch 480, Loss = 3.2577\n",
      "Epoch 5, Batch 490, Loss = 3.1752\n",
      "Epoch 5, Batch 500, Loss = 3.2102\n",
      "Epoch 5, Batch 510, Loss = 3.1572\n",
      "Epoch 5, Batch 520, Loss = 3.3112\n",
      "Epoch 5, Batch 530, Loss = 3.3037\n",
      "Epoch 5, Batch 540, Loss = 3.3319\n",
      "Epoch 5, Batch 550, Loss = 3.1844\n",
      "Epoch 5, Batch 560, Loss = 3.2389\n",
      "Epoch 5, Batch 570, Loss = 3.2982\n",
      "Epoch 5, Batch 580, Loss = 3.2163\n",
      "Epoch 5, Batch 590, Loss = 3.1943\n",
      "Epoch 5, Batch 600, Loss = 3.2040\n",
      "Epoch 5, Batch 610, Loss = 3.2773\n",
      "Epoch 5, Batch 620, Loss = 3.2794\n",
      "Epoch 5, Batch 630, Loss = 3.0720\n",
      "Epoch 5, Batch 640, Loss = 3.0479\n",
      "Epoch 5, Batch 650, Loss = 3.1671\n",
      "Epoch 5, Batch 660, Loss = 3.0273\n",
      "Epoch 5, Batch 670, Loss = 3.2115\n",
      "Epoch 5, Batch 680, Loss = 3.2552\n",
      "Epoch 5, Batch 690, Loss = 3.2603\n",
      "Epoch 5, Batch 700, Loss = 3.2209\n",
      "Epoch 5, Batch 710, Loss = 3.1203\n",
      "Epoch 5, Batch 720, Loss = 3.1546\n",
      "Epoch 5, Batch 730, Loss = 3.3215\n",
      "Epoch 5, Batch 740, Loss = 3.2700\n",
      "Epoch 5, Batch 750, Loss = 3.2027\n",
      "Epoch 5, Batch 760, Loss = 3.2023\n",
      "Epoch 5, Batch 770, Loss = 3.3239\n",
      "Epoch 5, Batch 780, Loss = 3.2563\n",
      "Epoch 5, Batch 790, Loss = 3.0969\n",
      "Epoch 5, Batch 800, Loss = 3.1682\n",
      "Epoch 5, Batch 810, Loss = 3.1028\n",
      "Epoch 5, Batch 820, Loss = 3.3144\n",
      "Epoch 5, Batch 830, Loss = 3.3546\n",
      "Epoch 5, Batch 840, Loss = 3.2061\n",
      "Epoch 5, Batch 850, Loss = 3.1923\n",
      "Epoch 5, Batch 860, Loss = 3.1892\n",
      "Epoch 5, Batch 870, Loss = 3.2413\n",
      "Epoch 5, Batch 880, Loss = 3.2108\n",
      "Epoch 5, Batch 890, Loss = 3.1452\n",
      "Epoch 5, Batch 900, Loss = 3.3396\n",
      "Epoch 5, Batch 910, Loss = 3.1932\n",
      "Epoch 5, Batch 920, Loss = 3.3665\n",
      "Epoch 5, Batch 930, Loss = 3.2520\n",
      "Epoch 5, Batch 940, Loss = 3.1781\n",
      "Epoch 5, Batch 950, Loss = 3.3052\n",
      "Epoch 5, Batch 960, Loss = 3.3499\n",
      "Epoch 5, Batch 970, Loss = 3.2872\n",
      "Epoch 5, Batch 980, Loss = 3.2469\n",
      "Epoch 5, Batch 990, Loss = 3.1105\n",
      "Epoch 5, Batch 1000, Loss = 3.1960\n",
      "Epoch 5, Batch 1010, Loss = 3.2848\n",
      "Epoch 5, Batch 1020, Loss = 3.3248\n",
      "Epoch 5, Batch 1030, Loss = 3.2344\n",
      "Epoch 5, Batch 1040, Loss = 3.1216\n",
      "Epoch 5, Batch 1050, Loss = 3.0828\n",
      "Epoch 5, Batch 1060, Loss = 3.2860\n",
      "Epoch 5, Batch 1070, Loss = 3.2400\n",
      "Epoch 5, Batch 1080, Loss = 3.2861\n",
      "Epoch 5, Batch 1090, Loss = 3.2455\n",
      "Epoch 5, Batch 1100, Loss = 3.0694\n",
      "Epoch 5, Batch 1110, Loss = 3.1822\n",
      "Epoch 5, Batch 1120, Loss = 3.1655\n",
      "Epoch 5, Average Loss = 3.2078\n",
      "Epoch 6, Batch 0, Loss = 3.1030\n",
      "Epoch 6, Batch 10, Loss = 3.3334\n",
      "Epoch 6, Batch 20, Loss = 3.0921\n",
      "Epoch 6, Batch 30, Loss = 3.0608\n",
      "Epoch 6, Batch 40, Loss = 3.1528\n",
      "Epoch 6, Batch 50, Loss = 3.2274\n",
      "Epoch 6, Batch 60, Loss = 3.1530\n",
      "Epoch 6, Batch 70, Loss = 3.2401\n",
      "Epoch 6, Batch 80, Loss = 3.3097\n",
      "Epoch 6, Batch 90, Loss = 3.0941\n",
      "Epoch 6, Batch 100, Loss = 3.2199\n",
      "Epoch 6, Batch 110, Loss = 3.2640\n",
      "Epoch 6, Batch 120, Loss = 3.2200\n",
      "Epoch 6, Batch 130, Loss = 3.1742\n",
      "Epoch 6, Batch 140, Loss = 3.1820\n",
      "Epoch 6, Batch 150, Loss = 3.1867\n",
      "Epoch 6, Batch 160, Loss = 3.1152\n",
      "Epoch 6, Batch 170, Loss = 3.1723\n",
      "Epoch 6, Batch 180, Loss = 3.2533\n",
      "Epoch 6, Batch 190, Loss = 3.0798\n",
      "Epoch 6, Batch 200, Loss = 3.1961\n",
      "Epoch 6, Batch 210, Loss = 3.2165\n",
      "Epoch 6, Batch 220, Loss = 3.1572\n",
      "Epoch 6, Batch 230, Loss = 3.0860\n",
      "Epoch 6, Batch 240, Loss = 3.1798\n",
      "Epoch 6, Batch 250, Loss = 3.2484\n",
      "Epoch 6, Batch 260, Loss = 3.1326\n",
      "Epoch 6, Batch 270, Loss = 3.1295\n",
      "Epoch 6, Batch 280, Loss = 3.2528\n",
      "Epoch 6, Batch 290, Loss = 3.3066\n",
      "Epoch 6, Batch 300, Loss = 3.1934\n",
      "Epoch 6, Batch 310, Loss = 3.1568\n",
      "Epoch 6, Batch 320, Loss = 3.2161\n",
      "Epoch 6, Batch 330, Loss = 3.1015\n",
      "Epoch 6, Batch 340, Loss = 3.2860\n",
      "Epoch 6, Batch 350, Loss = 3.1579\n",
      "Epoch 6, Batch 360, Loss = 3.0718\n",
      "Epoch 6, Batch 370, Loss = 3.1778\n",
      "Epoch 6, Batch 380, Loss = 3.1733\n",
      "Epoch 6, Batch 390, Loss = 3.2934\n",
      "Epoch 6, Batch 400, Loss = 3.0340\n",
      "Epoch 6, Batch 410, Loss = 3.1970\n",
      "Epoch 6, Batch 420, Loss = 3.2203\n",
      "Epoch 6, Batch 430, Loss = 3.3888\n",
      "Epoch 6, Batch 440, Loss = 3.2447\n",
      "Epoch 6, Batch 450, Loss = 3.1248\n",
      "Epoch 6, Batch 460, Loss = 3.1612\n",
      "Epoch 6, Batch 470, Loss = 3.0970\n",
      "Epoch 6, Batch 480, Loss = 3.2971\n",
      "Epoch 6, Batch 490, Loss = 3.1873\n",
      "Epoch 6, Batch 500, Loss = 3.2286\n",
      "Epoch 6, Batch 510, Loss = 3.0635\n",
      "Epoch 6, Batch 520, Loss = 3.1831\n",
      "Epoch 6, Batch 530, Loss = 3.3091\n",
      "Epoch 6, Batch 540, Loss = 3.2437\n",
      "Epoch 6, Batch 550, Loss = 3.1702\n",
      "Epoch 6, Batch 560, Loss = 3.1137\n",
      "Epoch 6, Batch 570, Loss = 3.0719\n",
      "Epoch 6, Batch 580, Loss = 3.2943\n",
      "Epoch 6, Batch 590, Loss = 3.2039\n",
      "Epoch 6, Batch 600, Loss = 3.2626\n",
      "Epoch 6, Batch 610, Loss = 3.3258\n",
      "Epoch 6, Batch 620, Loss = 3.1363\n",
      "Epoch 6, Batch 630, Loss = 3.2035\n",
      "Epoch 6, Batch 640, Loss = 3.2033\n",
      "Epoch 6, Batch 650, Loss = 3.1508\n",
      "Epoch 6, Batch 660, Loss = 3.2485\n",
      "Epoch 6, Batch 670, Loss = 3.0282\n",
      "Epoch 6, Batch 680, Loss = 3.1708\n",
      "Epoch 6, Batch 690, Loss = 3.3194\n",
      "Epoch 6, Batch 700, Loss = 3.1478\n",
      "Epoch 6, Batch 710, Loss = 3.2915\n",
      "Epoch 6, Batch 720, Loss = 3.0321\n",
      "Epoch 6, Batch 730, Loss = 3.3042\n",
      "Epoch 6, Batch 740, Loss = 3.2410\n",
      "Epoch 6, Batch 750, Loss = 3.2162\n",
      "Epoch 6, Batch 760, Loss = 3.2238\n",
      "Epoch 6, Batch 770, Loss = 3.1378\n",
      "Epoch 6, Batch 780, Loss = 3.1816\n",
      "Epoch 6, Batch 790, Loss = 3.4140\n",
      "Epoch 6, Batch 800, Loss = 3.2925\n",
      "Epoch 6, Batch 810, Loss = 3.1195\n",
      "Epoch 6, Batch 820, Loss = 3.3044\n",
      "Epoch 6, Batch 830, Loss = 3.0993\n",
      "Epoch 6, Batch 840, Loss = 3.3029\n",
      "Epoch 6, Batch 850, Loss = 3.2956\n",
      "Epoch 6, Batch 860, Loss = 3.2655\n",
      "Epoch 6, Batch 870, Loss = 3.1063\n",
      "Epoch 6, Batch 880, Loss = 3.3253\n",
      "Epoch 6, Batch 890, Loss = 3.2737\n",
      "Epoch 6, Batch 900, Loss = 3.2049\n",
      "Epoch 6, Batch 910, Loss = 3.3027\n",
      "Epoch 6, Batch 920, Loss = 3.1286\n",
      "Epoch 6, Batch 930, Loss = 3.2938\n",
      "Epoch 6, Batch 940, Loss = 3.2594\n",
      "Epoch 6, Batch 950, Loss = 2.8749\n",
      "Epoch 6, Batch 960, Loss = 3.0899\n",
      "Epoch 6, Batch 970, Loss = 3.3307\n",
      "Epoch 6, Batch 980, Loss = 3.2188\n",
      "Epoch 6, Batch 990, Loss = 3.0514\n",
      "Epoch 6, Batch 1000, Loss = 3.2346\n",
      "Epoch 6, Batch 1010, Loss = 3.1490\n",
      "Epoch 6, Batch 1020, Loss = 3.3515\n",
      "Epoch 6, Batch 1030, Loss = 3.2328\n",
      "Epoch 6, Batch 1040, Loss = 3.2729\n",
      "Epoch 6, Batch 1050, Loss = 3.3437\n",
      "Epoch 6, Batch 1060, Loss = 3.1096\n",
      "Epoch 6, Batch 1070, Loss = 3.1476\n",
      "Epoch 6, Batch 1080, Loss = 3.3728\n",
      "Epoch 6, Batch 1090, Loss = 3.0709\n",
      "Epoch 6, Batch 1100, Loss = 2.9897\n",
      "Epoch 6, Batch 1110, Loss = 3.0392\n",
      "Epoch 6, Batch 1120, Loss = 3.3054\n",
      "Epoch 6, Average Loss = 3.2007\n",
      "Epoch 7, Batch 0, Loss = 3.2604\n",
      "Epoch 7, Batch 10, Loss = 3.2152\n",
      "Epoch 7, Batch 20, Loss = 3.2235\n",
      "Epoch 7, Batch 30, Loss = 3.1940\n",
      "Epoch 7, Batch 40, Loss = 3.1776\n",
      "Epoch 7, Batch 50, Loss = 3.1648\n",
      "Epoch 7, Batch 60, Loss = 3.2536\n",
      "Epoch 7, Batch 70, Loss = 3.1387\n",
      "Epoch 7, Batch 80, Loss = 3.1623\n",
      "Epoch 7, Batch 90, Loss = 3.2092\n",
      "Epoch 7, Batch 100, Loss = 3.2973\n",
      "Epoch 7, Batch 110, Loss = 3.2189\n",
      "Epoch 7, Batch 120, Loss = 3.2481\n",
      "Epoch 7, Batch 130, Loss = 3.3380\n",
      "Epoch 7, Batch 140, Loss = 3.2159\n",
      "Epoch 7, Batch 150, Loss = 3.0666\n",
      "Epoch 7, Batch 160, Loss = 3.3489\n",
      "Epoch 7, Batch 170, Loss = 3.3405\n",
      "Epoch 7, Batch 180, Loss = 3.2967\n",
      "Epoch 7, Batch 190, Loss = 3.0539\n",
      "Epoch 7, Batch 200, Loss = 3.2901\n",
      "Epoch 7, Batch 210, Loss = 3.2133\n",
      "Epoch 7, Batch 220, Loss = 3.1580\n",
      "Epoch 7, Batch 230, Loss = 3.0849\n",
      "Epoch 7, Batch 240, Loss = 3.1861\n",
      "Epoch 7, Batch 250, Loss = 3.3272\n",
      "Epoch 7, Batch 260, Loss = 3.2239\n",
      "Epoch 7, Batch 270, Loss = 3.0681\n",
      "Epoch 7, Batch 280, Loss = 3.2442\n",
      "Epoch 7, Batch 290, Loss = 3.0941\n",
      "Epoch 7, Batch 300, Loss = 3.0617\n",
      "Epoch 7, Batch 310, Loss = 3.3378\n",
      "Epoch 7, Batch 320, Loss = 3.2817\n",
      "Epoch 7, Batch 330, Loss = 3.3730\n",
      "Epoch 7, Batch 340, Loss = 3.3004\n",
      "Epoch 7, Batch 350, Loss = 3.1875\n",
      "Epoch 7, Batch 360, Loss = 3.2446\n",
      "Epoch 7, Batch 370, Loss = 3.0445\n",
      "Epoch 7, Batch 380, Loss = 3.1401\n",
      "Epoch 7, Batch 390, Loss = 3.1655\n",
      "Epoch 7, Batch 400, Loss = 3.0730\n",
      "Epoch 7, Batch 410, Loss = 3.3523\n",
      "Epoch 7, Batch 420, Loss = 3.1322\n",
      "Epoch 7, Batch 430, Loss = 3.1061\n",
      "Epoch 7, Batch 440, Loss = 3.0912\n",
      "Epoch 7, Batch 450, Loss = 3.1252\n",
      "Epoch 7, Batch 460, Loss = 3.1147\n",
      "Epoch 7, Batch 470, Loss = 3.1972\n",
      "Epoch 7, Batch 480, Loss = 3.2172\n",
      "Epoch 7, Batch 490, Loss = 3.3191\n",
      "Epoch 7, Batch 500, Loss = 3.2900\n",
      "Epoch 7, Batch 510, Loss = 3.3163\n",
      "Epoch 7, Batch 520, Loss = 3.2535\n",
      "Epoch 7, Batch 530, Loss = 3.2077\n",
      "Epoch 7, Batch 540, Loss = 3.2405\n",
      "Epoch 7, Batch 550, Loss = 3.2296\n",
      "Epoch 7, Batch 560, Loss = 3.1285\n",
      "Epoch 7, Batch 570, Loss = 3.1353\n",
      "Epoch 7, Batch 580, Loss = 3.4263\n",
      "Epoch 7, Batch 590, Loss = 3.1867\n",
      "Epoch 7, Batch 600, Loss = 3.2048\n",
      "Epoch 7, Batch 610, Loss = 3.2325\n",
      "Epoch 7, Batch 620, Loss = 3.1682\n",
      "Epoch 7, Batch 630, Loss = 3.1227\n",
      "Epoch 7, Batch 640, Loss = 3.2540\n",
      "Epoch 7, Batch 650, Loss = 3.1626\n",
      "Epoch 7, Batch 660, Loss = 3.2849\n",
      "Epoch 7, Batch 670, Loss = 3.2187\n",
      "Epoch 7, Batch 680, Loss = 3.0905\n",
      "Epoch 7, Batch 690, Loss = 3.2245\n",
      "Epoch 7, Batch 700, Loss = 3.3128\n",
      "Epoch 7, Batch 710, Loss = 3.1012\n",
      "Epoch 7, Batch 720, Loss = 3.1386\n",
      "Epoch 7, Batch 730, Loss = 3.1180\n",
      "Epoch 7, Batch 740, Loss = 3.2266\n",
      "Epoch 7, Batch 750, Loss = 3.2529\n",
      "Epoch 7, Batch 760, Loss = 3.2454\n",
      "Epoch 7, Batch 770, Loss = 3.1246\n",
      "Epoch 7, Batch 780, Loss = 3.1815\n",
      "Epoch 7, Batch 790, Loss = 3.0610\n",
      "Epoch 7, Batch 800, Loss = 3.1010\n",
      "Epoch 7, Batch 810, Loss = 3.2422\n",
      "Epoch 7, Batch 820, Loss = 3.1050\n",
      "Epoch 7, Batch 830, Loss = 3.0171\n",
      "Epoch 7, Batch 840, Loss = 3.2166\n",
      "Epoch 7, Batch 850, Loss = 3.1901\n",
      "Epoch 7, Batch 860, Loss = 3.1892\n",
      "Epoch 7, Batch 870, Loss = 3.1969\n",
      "Epoch 7, Batch 880, Loss = 3.3524\n",
      "Epoch 7, Batch 890, Loss = 3.2423\n",
      "Epoch 7, Batch 900, Loss = 3.1747\n",
      "Epoch 7, Batch 910, Loss = 3.0810\n",
      "Epoch 7, Batch 920, Loss = 3.4005\n",
      "Epoch 7, Batch 930, Loss = 3.2469\n",
      "Epoch 7, Batch 940, Loss = 3.2681\n",
      "Epoch 7, Batch 950, Loss = 3.1675\n",
      "Epoch 7, Batch 960, Loss = 3.2614\n",
      "Epoch 7, Batch 970, Loss = 3.2210\n",
      "Epoch 7, Batch 980, Loss = 3.2236\n",
      "Epoch 7, Batch 990, Loss = 3.1993\n",
      "Epoch 7, Batch 1000, Loss = 3.0245\n",
      "Epoch 7, Batch 1010, Loss = 3.0875\n",
      "Epoch 7, Batch 1020, Loss = 3.3953\n",
      "Epoch 7, Batch 1030, Loss = 3.1351\n",
      "Epoch 7, Batch 1040, Loss = 3.3164\n",
      "Epoch 7, Batch 1050, Loss = 3.2403\n",
      "Epoch 7, Batch 1060, Loss = 3.1289\n",
      "Epoch 7, Batch 1070, Loss = 3.0809\n",
      "Epoch 7, Batch 1080, Loss = 3.2074\n",
      "Epoch 7, Batch 1090, Loss = 3.2520\n",
      "Epoch 7, Batch 1100, Loss = 3.1548\n",
      "Epoch 7, Batch 1110, Loss = 3.2097\n",
      "Epoch 7, Batch 1120, Loss = 3.2752\n",
      "Epoch 7, Average Loss = 3.1946\n",
      "Epoch 8, Batch 0, Loss = 3.1890\n",
      "Epoch 8, Batch 10, Loss = 3.3034\n",
      "Epoch 8, Batch 20, Loss = 3.0273\n",
      "Epoch 8, Batch 30, Loss = 3.0450\n",
      "Epoch 8, Batch 40, Loss = 3.1626\n",
      "Epoch 8, Batch 50, Loss = 3.1996\n",
      "Epoch 8, Batch 60, Loss = 3.1982\n",
      "Epoch 8, Batch 70, Loss = 3.1521\n",
      "Epoch 8, Batch 80, Loss = 3.1906\n",
      "Epoch 8, Batch 90, Loss = 3.0774\n",
      "Epoch 8, Batch 100, Loss = 3.2023\n",
      "Epoch 8, Batch 110, Loss = 3.1544\n",
      "Epoch 8, Batch 120, Loss = 3.2262\n",
      "Epoch 8, Batch 130, Loss = 3.1825\n",
      "Epoch 8, Batch 140, Loss = 3.2267\n",
      "Epoch 8, Batch 150, Loss = 3.2911\n",
      "Epoch 8, Batch 160, Loss = 3.2887\n",
      "Epoch 8, Batch 170, Loss = 3.0838\n",
      "Epoch 8, Batch 180, Loss = 3.1472\n",
      "Epoch 8, Batch 190, Loss = 3.2837\n",
      "Epoch 8, Batch 200, Loss = 3.1335\n",
      "Epoch 8, Batch 210, Loss = 3.1013\n",
      "Epoch 8, Batch 220, Loss = 3.2157\n",
      "Epoch 8, Batch 230, Loss = 3.0965\n",
      "Epoch 8, Batch 240, Loss = 3.1332\n",
      "Epoch 8, Batch 250, Loss = 3.2706\n",
      "Epoch 8, Batch 260, Loss = 3.4077\n",
      "Epoch 8, Batch 270, Loss = 3.2217\n",
      "Epoch 8, Batch 280, Loss = 3.1758\n",
      "Epoch 8, Batch 290, Loss = 3.2560\n",
      "Epoch 8, Batch 300, Loss = 3.1740\n",
      "Epoch 8, Batch 310, Loss = 3.3025\n",
      "Epoch 8, Batch 320, Loss = 3.0702\n",
      "Epoch 8, Batch 330, Loss = 3.2251\n",
      "Epoch 8, Batch 340, Loss = 3.2559\n",
      "Epoch 8, Batch 350, Loss = 3.0563\n",
      "Epoch 8, Batch 360, Loss = 3.2414\n",
      "Epoch 8, Batch 370, Loss = 3.2839\n",
      "Epoch 8, Batch 380, Loss = 3.2360\n",
      "Epoch 8, Batch 390, Loss = 3.1975\n",
      "Epoch 8, Batch 400, Loss = 3.2523\n",
      "Epoch 8, Batch 410, Loss = 3.1136\n",
      "Epoch 8, Batch 420, Loss = 3.0759\n",
      "Epoch 8, Batch 430, Loss = 3.1396\n",
      "Epoch 8, Batch 440, Loss = 3.2927\n",
      "Epoch 8, Batch 450, Loss = 3.3627\n",
      "Epoch 8, Batch 460, Loss = 3.2678\n",
      "Epoch 8, Batch 470, Loss = 3.2600\n",
      "Epoch 8, Batch 480, Loss = 3.3713\n",
      "Epoch 8, Batch 490, Loss = 3.2475\n",
      "Epoch 8, Batch 500, Loss = 3.1121\n",
      "Epoch 8, Batch 510, Loss = 3.0405\n",
      "Epoch 8, Batch 520, Loss = 3.1108\n",
      "Epoch 8, Batch 530, Loss = 3.2143\n",
      "Epoch 8, Batch 540, Loss = 3.1034\n",
      "Epoch 8, Batch 550, Loss = 3.1704\n",
      "Epoch 8, Batch 560, Loss = 3.0961\n",
      "Epoch 8, Batch 570, Loss = 3.3572\n",
      "Epoch 8, Batch 580, Loss = 3.2265\n",
      "Epoch 8, Batch 590, Loss = 3.2758\n",
      "Epoch 8, Batch 600, Loss = 3.1374\n",
      "Epoch 8, Batch 610, Loss = 3.1230\n",
      "Epoch 8, Batch 620, Loss = 3.1316\n",
      "Epoch 8, Batch 630, Loss = 3.3003\n",
      "Epoch 8, Batch 640, Loss = 3.1306\n",
      "Epoch 8, Batch 650, Loss = 3.0555\n",
      "Epoch 8, Batch 660, Loss = 3.2638\n",
      "Epoch 8, Batch 670, Loss = 3.1681\n",
      "Epoch 8, Batch 680, Loss = 3.1982\n",
      "Epoch 8, Batch 690, Loss = 3.1589\n",
      "Epoch 8, Batch 700, Loss = 3.1550\n",
      "Epoch 8, Batch 710, Loss = 3.1036\n",
      "Epoch 8, Batch 720, Loss = 3.1599\n",
      "Epoch 8, Batch 730, Loss = 2.9779\n",
      "Epoch 8, Batch 740, Loss = 3.3213\n",
      "Epoch 8, Batch 750, Loss = 3.2228\n",
      "Epoch 8, Batch 760, Loss = 3.0790\n",
      "Epoch 8, Batch 770, Loss = 3.2818\n",
      "Epoch 8, Batch 780, Loss = 3.2623\n",
      "Epoch 8, Batch 790, Loss = 3.0762\n",
      "Epoch 8, Batch 800, Loss = 3.4264\n",
      "Epoch 8, Batch 810, Loss = 3.2158\n",
      "Epoch 8, Batch 820, Loss = 3.2958\n",
      "Epoch 8, Batch 830, Loss = 3.3646\n",
      "Epoch 8, Batch 840, Loss = 3.1051\n",
      "Epoch 8, Batch 850, Loss = 2.9765\n",
      "Epoch 8, Batch 860, Loss = 3.1613\n",
      "Epoch 8, Batch 870, Loss = 3.1584\n",
      "Epoch 8, Batch 880, Loss = 3.1950\n",
      "Epoch 8, Batch 890, Loss = 3.3050\n",
      "Epoch 8, Batch 900, Loss = 3.0834\n",
      "Epoch 8, Batch 910, Loss = 3.4053\n",
      "Epoch 8, Batch 920, Loss = 3.2447\n",
      "Epoch 8, Batch 930, Loss = 3.2491\n",
      "Epoch 8, Batch 940, Loss = 3.2023\n",
      "Epoch 8, Batch 950, Loss = 3.1843\n",
      "Epoch 8, Batch 960, Loss = 3.2169\n",
      "Epoch 8, Batch 970, Loss = 3.0530\n",
      "Epoch 8, Batch 980, Loss = 3.1590\n",
      "Epoch 8, Batch 990, Loss = 3.1058\n",
      "Epoch 8, Batch 1000, Loss = 3.2460\n",
      "Epoch 8, Batch 1010, Loss = 3.1493\n",
      "Epoch 8, Batch 1020, Loss = 3.0719\n",
      "Epoch 8, Batch 1030, Loss = 3.1843\n",
      "Epoch 8, Batch 1040, Loss = 3.2185\n",
      "Epoch 8, Batch 1050, Loss = 3.1693\n",
      "Epoch 8, Batch 1060, Loss = 3.1243\n",
      "Epoch 8, Batch 1070, Loss = 3.1706\n",
      "Epoch 8, Batch 1080, Loss = 3.1886\n",
      "Epoch 8, Batch 1090, Loss = 3.2627\n",
      "Epoch 8, Batch 1100, Loss = 3.3301\n",
      "Epoch 8, Batch 1110, Loss = 3.0488\n",
      "Epoch 8, Batch 1120, Loss = 3.0886\n",
      "Epoch 8, Average Loss = 3.1884\n",
      "Epoch 9, Batch 0, Loss = 3.0182\n",
      "Epoch 9, Batch 10, Loss = 3.1459\n",
      "Epoch 9, Batch 20, Loss = 3.0150\n",
      "Epoch 9, Batch 30, Loss = 3.1389\n",
      "Epoch 9, Batch 40, Loss = 3.1248\n",
      "Epoch 9, Batch 50, Loss = 3.2650\n",
      "Epoch 9, Batch 60, Loss = 2.9785\n",
      "Epoch 9, Batch 70, Loss = 3.1886\n",
      "Epoch 9, Batch 80, Loss = 3.2360\n",
      "Epoch 9, Batch 90, Loss = 3.2880\n",
      "Epoch 9, Batch 100, Loss = 3.2301\n",
      "Epoch 9, Batch 110, Loss = 3.2373\n",
      "Epoch 9, Batch 120, Loss = 3.1654\n",
      "Epoch 9, Batch 130, Loss = 3.2213\n",
      "Epoch 9, Batch 140, Loss = 3.1102\n",
      "Epoch 9, Batch 150, Loss = 3.0389\n",
      "Epoch 9, Batch 160, Loss = 3.3092\n",
      "Epoch 9, Batch 170, Loss = 3.1272\n",
      "Epoch 9, Batch 180, Loss = 3.2324\n",
      "Epoch 9, Batch 190, Loss = 3.0910\n",
      "Epoch 9, Batch 200, Loss = 3.1234\n",
      "Epoch 9, Batch 210, Loss = 3.1981\n",
      "Epoch 9, Batch 220, Loss = 3.2376\n",
      "Epoch 9, Batch 230, Loss = 3.2032\n",
      "Epoch 9, Batch 240, Loss = 3.1298\n",
      "Epoch 9, Batch 250, Loss = 3.1663\n",
      "Epoch 9, Batch 260, Loss = 3.0145\n",
      "Epoch 9, Batch 270, Loss = 3.1745\n",
      "Epoch 9, Batch 280, Loss = 3.2659\n",
      "Epoch 9, Batch 290, Loss = 3.2309\n",
      "Epoch 9, Batch 300, Loss = 3.1491\n",
      "Epoch 9, Batch 310, Loss = 3.3021\n",
      "Epoch 9, Batch 320, Loss = 3.1425\n",
      "Epoch 9, Batch 330, Loss = 3.2035\n",
      "Epoch 9, Batch 340, Loss = 3.2232\n",
      "Epoch 9, Batch 350, Loss = 3.1593\n",
      "Epoch 9, Batch 360, Loss = 3.2192\n",
      "Epoch 9, Batch 370, Loss = 3.2090\n",
      "Epoch 9, Batch 380, Loss = 3.1331\n",
      "Epoch 9, Batch 390, Loss = 3.2217\n",
      "Epoch 9, Batch 400, Loss = 3.0840\n",
      "Epoch 9, Batch 410, Loss = 3.2614\n",
      "Epoch 9, Batch 420, Loss = 3.2199\n",
      "Epoch 9, Batch 430, Loss = 3.1690\n",
      "Epoch 9, Batch 440, Loss = 3.1595\n",
      "Epoch 9, Batch 450, Loss = 3.3124\n",
      "Epoch 9, Batch 460, Loss = 3.1480\n",
      "Epoch 9, Batch 470, Loss = 3.2436\n",
      "Epoch 9, Batch 480, Loss = 3.3622\n",
      "Epoch 9, Batch 490, Loss = 3.0124\n",
      "Epoch 9, Batch 500, Loss = 3.2328\n",
      "Epoch 9, Batch 510, Loss = 3.2634\n",
      "Epoch 9, Batch 520, Loss = 3.1100\n",
      "Epoch 9, Batch 530, Loss = 3.2417\n",
      "Epoch 9, Batch 540, Loss = 3.2361\n",
      "Epoch 9, Batch 550, Loss = 3.2641\n",
      "Epoch 9, Batch 560, Loss = 3.0430\n",
      "Epoch 9, Batch 570, Loss = 3.1872\n",
      "Epoch 9, Batch 580, Loss = 3.0584\n",
      "Epoch 9, Batch 590, Loss = 3.2033\n",
      "Epoch 9, Batch 600, Loss = 3.1884\n",
      "Epoch 9, Batch 610, Loss = 3.1578\n",
      "Epoch 9, Batch 620, Loss = 3.1406\n",
      "Epoch 9, Batch 630, Loss = 3.1255\n",
      "Epoch 9, Batch 640, Loss = 3.1985\n",
      "Epoch 9, Batch 650, Loss = 3.4567\n",
      "Epoch 9, Batch 660, Loss = 3.2114\n",
      "Epoch 9, Batch 670, Loss = 3.3262\n",
      "Epoch 9, Batch 680, Loss = 3.2322\n",
      "Epoch 9, Batch 690, Loss = 3.0860\n",
      "Epoch 9, Batch 700, Loss = 3.2837\n",
      "Epoch 9, Batch 710, Loss = 3.2935\n",
      "Epoch 9, Batch 720, Loss = 3.2039\n",
      "Epoch 9, Batch 730, Loss = 3.2561\n",
      "Epoch 9, Batch 740, Loss = 3.1756\n",
      "Epoch 9, Batch 750, Loss = 3.3189\n",
      "Epoch 9, Batch 760, Loss = 3.2054\n",
      "Epoch 9, Batch 770, Loss = 3.1797\n",
      "Epoch 9, Batch 780, Loss = 3.1104\n",
      "Epoch 9, Batch 790, Loss = 3.2037\n",
      "Epoch 9, Batch 800, Loss = 3.2560\n",
      "Epoch 9, Batch 810, Loss = 3.1430\n",
      "Epoch 9, Batch 820, Loss = 3.2881\n",
      "Epoch 9, Batch 830, Loss = 3.0776\n",
      "Epoch 9, Batch 840, Loss = 3.2142\n",
      "Epoch 9, Batch 850, Loss = 3.1920\n",
      "Epoch 9, Batch 860, Loss = 3.3782\n",
      "Epoch 9, Batch 870, Loss = 3.2316\n",
      "Epoch 9, Batch 880, Loss = 3.1660\n",
      "Epoch 9, Batch 890, Loss = 3.1007\n",
      "Epoch 9, Batch 900, Loss = 3.0364\n",
      "Epoch 9, Batch 910, Loss = 3.1567\n",
      "Epoch 9, Batch 920, Loss = 3.2417\n",
      "Epoch 9, Batch 930, Loss = 2.9567\n",
      "Epoch 9, Batch 940, Loss = 3.2037\n",
      "Epoch 9, Batch 950, Loss = 3.2961\n",
      "Epoch 9, Batch 960, Loss = 3.2660\n",
      "Epoch 9, Batch 970, Loss = 3.2034\n",
      "Epoch 9, Batch 980, Loss = 3.0824\n",
      "Epoch 9, Batch 990, Loss = 3.2214\n",
      "Epoch 9, Batch 1000, Loss = 3.2394\n",
      "Epoch 9, Batch 1010, Loss = 3.0060\n",
      "Epoch 9, Batch 1020, Loss = 3.1533\n",
      "Epoch 9, Batch 1030, Loss = 3.1884\n",
      "Epoch 9, Batch 1040, Loss = 2.9298\n",
      "Epoch 9, Batch 1050, Loss = 3.2841\n",
      "Epoch 9, Batch 1060, Loss = 3.2280\n",
      "Epoch 9, Batch 1070, Loss = 3.2655\n",
      "Epoch 9, Batch 1080, Loss = 3.1735\n",
      "Epoch 9, Batch 1090, Loss = 2.9977\n",
      "Epoch 9, Batch 1100, Loss = 3.1259\n",
      "Epoch 9, Batch 1110, Loss = 3.1420\n",
      "Epoch 9, Batch 1120, Loss = 3.0892\n",
      "Epoch 9, Average Loss = 3.1813\n",
      "Epoch 10, Batch 0, Loss = 3.1788\n",
      "Epoch 10, Batch 10, Loss = 3.0806\n",
      "Epoch 10, Batch 20, Loss = 3.0738\n",
      "Epoch 10, Batch 30, Loss = 3.1655\n",
      "Epoch 10, Batch 40, Loss = 3.1338\n",
      "Epoch 10, Batch 50, Loss = 3.0681\n",
      "Epoch 10, Batch 60, Loss = 3.2650\n",
      "Epoch 10, Batch 70, Loss = 3.0767\n",
      "Epoch 10, Batch 80, Loss = 3.1012\n",
      "Epoch 10, Batch 90, Loss = 3.0719\n",
      "Epoch 10, Batch 100, Loss = 3.1486\n",
      "Epoch 10, Batch 110, Loss = 2.9884\n",
      "Epoch 10, Batch 120, Loss = 3.2667\n",
      "Epoch 10, Batch 130, Loss = 3.1500\n",
      "Epoch 10, Batch 140, Loss = 3.2295\n",
      "Epoch 10, Batch 150, Loss = 3.2828\n",
      "Epoch 10, Batch 160, Loss = 3.2231\n",
      "Epoch 10, Batch 170, Loss = 3.1272\n",
      "Epoch 10, Batch 180, Loss = 3.1288\n",
      "Epoch 10, Batch 190, Loss = 3.0752\n",
      "Epoch 10, Batch 200, Loss = 3.0956\n",
      "Epoch 10, Batch 210, Loss = 3.1358\n",
      "Epoch 10, Batch 220, Loss = 3.0193\n",
      "Epoch 10, Batch 230, Loss = 3.2383\n",
      "Epoch 10, Batch 240, Loss = 3.1683\n",
      "Epoch 10, Batch 250, Loss = 3.2316\n",
      "Epoch 10, Batch 260, Loss = 3.2465\n",
      "Epoch 10, Batch 270, Loss = 3.2163\n",
      "Epoch 10, Batch 280, Loss = 3.3480\n",
      "Epoch 10, Batch 290, Loss = 3.2756\n",
      "Epoch 10, Batch 300, Loss = 3.1876\n",
      "Epoch 10, Batch 310, Loss = 2.9545\n",
      "Epoch 10, Batch 320, Loss = 3.2062\n",
      "Epoch 10, Batch 330, Loss = 3.2475\n",
      "Epoch 10, Batch 340, Loss = 3.2592\n",
      "Epoch 10, Batch 350, Loss = 3.1443\n",
      "Epoch 10, Batch 360, Loss = 3.1726\n",
      "Epoch 10, Batch 370, Loss = 3.0514\n",
      "Epoch 10, Batch 380, Loss = 3.1622\n",
      "Epoch 10, Batch 390, Loss = 3.1432\n",
      "Epoch 10, Batch 400, Loss = 2.9735\n",
      "Epoch 10, Batch 410, Loss = 3.2363\n",
      "Epoch 10, Batch 420, Loss = 3.0614\n",
      "Epoch 10, Batch 430, Loss = 3.2088\n",
      "Epoch 10, Batch 440, Loss = 3.1512\n",
      "Epoch 10, Batch 450, Loss = 3.2716\n",
      "Epoch 10, Batch 460, Loss = 3.1456\n",
      "Epoch 10, Batch 470, Loss = 3.3408\n",
      "Epoch 10, Batch 480, Loss = 3.1456\n",
      "Epoch 10, Batch 490, Loss = 3.1908\n",
      "Epoch 10, Batch 500, Loss = 3.3220\n",
      "Epoch 10, Batch 510, Loss = 3.1102\n",
      "Epoch 10, Batch 520, Loss = 3.0920\n",
      "Epoch 10, Batch 530, Loss = 3.3865\n",
      "Epoch 10, Batch 540, Loss = 3.0625\n",
      "Epoch 10, Batch 550, Loss = 3.1108\n",
      "Epoch 10, Batch 560, Loss = 3.3860\n",
      "Epoch 10, Batch 570, Loss = 3.2335\n",
      "Epoch 10, Batch 580, Loss = 3.1661\n",
      "Epoch 10, Batch 590, Loss = 3.2384\n",
      "Epoch 10, Batch 600, Loss = 3.2404\n",
      "Epoch 10, Batch 610, Loss = 3.2143\n",
      "Epoch 10, Batch 620, Loss = 3.1625\n",
      "Epoch 10, Batch 630, Loss = 3.1499\n",
      "Epoch 10, Batch 640, Loss = 3.2087\n",
      "Epoch 10, Batch 650, Loss = 3.2380\n",
      "Epoch 10, Batch 660, Loss = 3.1103\n",
      "Epoch 10, Batch 670, Loss = 3.3221\n",
      "Epoch 10, Batch 680, Loss = 3.0622\n",
      "Epoch 10, Batch 690, Loss = 3.3946\n",
      "Epoch 10, Batch 700, Loss = 3.2053\n",
      "Epoch 10, Batch 710, Loss = 3.2443\n",
      "Epoch 10, Batch 720, Loss = 3.1811\n",
      "Epoch 10, Batch 730, Loss = 3.2013\n",
      "Epoch 10, Batch 740, Loss = 3.3698\n",
      "Epoch 10, Batch 750, Loss = 3.3135\n",
      "Epoch 10, Batch 760, Loss = 3.2924\n",
      "Epoch 10, Batch 770, Loss = 3.2263\n",
      "Epoch 10, Batch 780, Loss = 3.2609\n",
      "Epoch 10, Batch 790, Loss = 3.1817\n",
      "Epoch 10, Batch 800, Loss = 3.1075\n",
      "Epoch 10, Batch 810, Loss = 3.2152\n",
      "Epoch 10, Batch 820, Loss = 3.3226\n",
      "Epoch 10, Batch 830, Loss = 3.3449\n",
      "Epoch 10, Batch 840, Loss = 3.2680\n",
      "Epoch 10, Batch 850, Loss = 3.2922\n",
      "Epoch 10, Batch 860, Loss = 3.2727\n",
      "Epoch 10, Batch 870, Loss = 3.1399\n",
      "Epoch 10, Batch 880, Loss = 3.2681\n",
      "Epoch 10, Batch 890, Loss = 3.1392\n",
      "Epoch 10, Batch 900, Loss = 3.1483\n",
      "Epoch 10, Batch 910, Loss = 3.1337\n",
      "Epoch 10, Batch 920, Loss = 3.0843\n",
      "Epoch 10, Batch 930, Loss = 2.9311\n",
      "Epoch 10, Batch 940, Loss = 3.1648\n",
      "Epoch 10, Batch 950, Loss = 3.1328\n",
      "Epoch 10, Batch 960, Loss = 3.1545\n",
      "Epoch 10, Batch 970, Loss = 3.2620\n",
      "Epoch 10, Batch 980, Loss = 3.2069\n",
      "Epoch 10, Batch 990, Loss = 3.1341\n",
      "Epoch 10, Batch 1000, Loss = 3.2104\n",
      "Epoch 10, Batch 1010, Loss = 3.1943\n",
      "Epoch 10, Batch 1020, Loss = 3.1632\n",
      "Epoch 10, Batch 1030, Loss = 3.1949\n",
      "Epoch 10, Batch 1040, Loss = 3.2323\n",
      "Epoch 10, Batch 1050, Loss = 3.0608\n",
      "Epoch 10, Batch 1060, Loss = 3.1788\n",
      "Epoch 10, Batch 1070, Loss = 3.1912\n",
      "Epoch 10, Batch 1080, Loss = 3.1515\n",
      "Epoch 10, Batch 1090, Loss = 3.2337\n",
      "Epoch 10, Batch 1100, Loss = 3.1950\n",
      "Epoch 10, Batch 1110, Loss = 3.2087\n",
      "Epoch 10, Batch 1120, Loss = 3.3102\n",
      "Epoch 10, Average Loss = 3.1784\n",
      "Epoch 11, Batch 0, Loss = 3.0668\n",
      "Epoch 11, Batch 10, Loss = 3.2511\n",
      "Epoch 11, Batch 20, Loss = 3.2112\n",
      "Epoch 11, Batch 30, Loss = 3.2754\n",
      "Epoch 11, Batch 40, Loss = 3.1310\n",
      "Epoch 11, Batch 50, Loss = 3.1048\n",
      "Epoch 11, Batch 60, Loss = 3.1079\n",
      "Epoch 11, Batch 70, Loss = 3.0455\n",
      "Epoch 11, Batch 80, Loss = 3.1424\n",
      "Epoch 11, Batch 90, Loss = 3.2197\n",
      "Epoch 11, Batch 100, Loss = 3.3030\n",
      "Epoch 11, Batch 110, Loss = 3.2873\n",
      "Epoch 11, Batch 120, Loss = 3.3449\n",
      "Epoch 11, Batch 130, Loss = 3.0362\n",
      "Epoch 11, Batch 140, Loss = 3.0709\n",
      "Epoch 11, Batch 150, Loss = 3.1540\n",
      "Epoch 11, Batch 160, Loss = 3.1663\n",
      "Epoch 11, Batch 170, Loss = 3.2548\n",
      "Epoch 11, Batch 180, Loss = 3.0362\n",
      "Epoch 11, Batch 190, Loss = 3.1973\n",
      "Epoch 11, Batch 200, Loss = 3.2249\n",
      "Epoch 11, Batch 210, Loss = 3.1992\n",
      "Epoch 11, Batch 220, Loss = 3.2098\n",
      "Epoch 11, Batch 230, Loss = 3.0171\n",
      "Epoch 11, Batch 240, Loss = 3.0110\n",
      "Epoch 11, Batch 250, Loss = 3.1501\n",
      "Epoch 11, Batch 260, Loss = 3.1750\n",
      "Epoch 11, Batch 270, Loss = 3.3032\n",
      "Epoch 11, Batch 280, Loss = 3.1335\n",
      "Epoch 11, Batch 290, Loss = 2.9633\n",
      "Epoch 11, Batch 300, Loss = 3.0609\n",
      "Epoch 11, Batch 310, Loss = 3.3430\n",
      "Epoch 11, Batch 320, Loss = 3.1758\n",
      "Epoch 11, Batch 330, Loss = 3.0996\n",
      "Epoch 11, Batch 340, Loss = 3.1729\n",
      "Epoch 11, Batch 350, Loss = 3.1144\n",
      "Epoch 11, Batch 360, Loss = 3.2009\n",
      "Epoch 11, Batch 370, Loss = 3.0089\n",
      "Epoch 11, Batch 380, Loss = 3.1798\n",
      "Epoch 11, Batch 390, Loss = 3.2343\n",
      "Epoch 11, Batch 400, Loss = 3.1668\n",
      "Epoch 11, Batch 410, Loss = 3.2371\n",
      "Epoch 11, Batch 420, Loss = 3.0323\n",
      "Epoch 11, Batch 430, Loss = 3.1271\n",
      "Epoch 11, Batch 440, Loss = 3.1333\n",
      "Epoch 11, Batch 450, Loss = 3.3234\n",
      "Epoch 11, Batch 460, Loss = 3.1919\n",
      "Epoch 11, Batch 470, Loss = 3.0915\n",
      "Epoch 11, Batch 480, Loss = 3.1440\n",
      "Epoch 11, Batch 490, Loss = 3.2274\n",
      "Epoch 11, Batch 500, Loss = 3.1653\n",
      "Epoch 11, Batch 510, Loss = 3.1400\n",
      "Epoch 11, Batch 520, Loss = 3.0540\n",
      "Epoch 11, Batch 530, Loss = 3.1979\n",
      "Epoch 11, Batch 540, Loss = 3.2033\n",
      "Epoch 11, Batch 550, Loss = 3.1476\n",
      "Epoch 11, Batch 560, Loss = 3.0875\n",
      "Epoch 11, Batch 570, Loss = 3.2288\n",
      "Epoch 11, Batch 580, Loss = 3.3279\n",
      "Epoch 11, Batch 590, Loss = 3.1890\n",
      "Epoch 11, Batch 600, Loss = 3.2228\n",
      "Epoch 11, Batch 610, Loss = 3.2148\n",
      "Epoch 11, Batch 620, Loss = 3.2548\n",
      "Epoch 11, Batch 630, Loss = 3.2251\n",
      "Epoch 11, Batch 640, Loss = 3.1836\n",
      "Epoch 11, Batch 650, Loss = 3.2099\n",
      "Epoch 11, Batch 660, Loss = 3.1379\n",
      "Epoch 11, Batch 670, Loss = 3.2188\n",
      "Epoch 11, Batch 680, Loss = 3.1765\n",
      "Epoch 11, Batch 690, Loss = 3.1872\n",
      "Epoch 11, Batch 700, Loss = 3.3890\n",
      "Epoch 11, Batch 710, Loss = 3.1193\n",
      "Epoch 11, Batch 720, Loss = 3.2107\n",
      "Epoch 11, Batch 730, Loss = 3.0589\n",
      "Epoch 11, Batch 740, Loss = 3.1657\n",
      "Epoch 11, Batch 750, Loss = 3.3240\n",
      "Epoch 11, Batch 760, Loss = 3.3030\n",
      "Epoch 11, Batch 770, Loss = 3.3287\n",
      "Epoch 11, Batch 780, Loss = 3.1671\n",
      "Epoch 11, Batch 790, Loss = 3.1054\n",
      "Epoch 11, Batch 800, Loss = 3.2201\n",
      "Epoch 11, Batch 810, Loss = 3.2038\n",
      "Epoch 11, Batch 820, Loss = 3.1911\n",
      "Epoch 11, Batch 830, Loss = 3.0548\n",
      "Epoch 11, Batch 840, Loss = 3.2626\n",
      "Epoch 11, Batch 850, Loss = 3.2255\n",
      "Epoch 11, Batch 860, Loss = 3.1669\n",
      "Epoch 11, Batch 870, Loss = 3.1222\n",
      "Epoch 11, Batch 880, Loss = 3.1146\n",
      "Epoch 11, Batch 890, Loss = 3.0786\n",
      "Epoch 11, Batch 900, Loss = 3.0877\n",
      "Epoch 11, Batch 910, Loss = 3.1171\n",
      "Epoch 11, Batch 920, Loss = 3.2146\n",
      "Epoch 11, Batch 930, Loss = 3.1334\n",
      "Epoch 11, Batch 940, Loss = 3.1989\n",
      "Epoch 11, Batch 950, Loss = 3.0731\n",
      "Epoch 11, Batch 960, Loss = 3.0244\n",
      "Epoch 11, Batch 970, Loss = 3.1328\n",
      "Epoch 11, Batch 980, Loss = 3.2329\n",
      "Epoch 11, Batch 990, Loss = 3.2571\n",
      "Epoch 11, Batch 1000, Loss = 3.1125\n",
      "Epoch 11, Batch 1010, Loss = 3.1823\n",
      "Epoch 11, Batch 1020, Loss = 3.2309\n",
      "Epoch 11, Batch 1030, Loss = 3.2459\n",
      "Epoch 11, Batch 1040, Loss = 3.0585\n",
      "Epoch 11, Batch 1050, Loss = 3.2144\n",
      "Epoch 11, Batch 1060, Loss = 3.3170\n",
      "Epoch 11, Batch 1070, Loss = 3.1174\n",
      "Epoch 11, Batch 1080, Loss = 3.1090\n",
      "Epoch 11, Batch 1090, Loss = 3.2380\n",
      "Epoch 11, Batch 1100, Loss = 3.2615\n",
      "Epoch 11, Batch 1110, Loss = 3.0262\n",
      "Epoch 11, Batch 1120, Loss = 3.1516\n",
      "Epoch 11, Average Loss = 3.1727\n",
      "Epoch 12, Batch 0, Loss = 3.0902\n",
      "Epoch 12, Batch 10, Loss = 3.1453\n",
      "Epoch 12, Batch 20, Loss = 3.3001\n",
      "Epoch 12, Batch 30, Loss = 3.1047\n",
      "Epoch 12, Batch 40, Loss = 3.0116\n",
      "Epoch 12, Batch 50, Loss = 2.9725\n",
      "Epoch 12, Batch 60, Loss = 3.1521\n",
      "Epoch 12, Batch 70, Loss = 3.1321\n",
      "Epoch 12, Batch 80, Loss = 3.1187\n",
      "Epoch 12, Batch 90, Loss = 3.2208\n",
      "Epoch 12, Batch 100, Loss = 3.2021\n",
      "Epoch 12, Batch 110, Loss = 3.2442\n",
      "Epoch 12, Batch 120, Loss = 3.0515\n",
      "Epoch 12, Batch 130, Loss = 3.2853\n",
      "Epoch 12, Batch 140, Loss = 3.2026\n",
      "Epoch 12, Batch 150, Loss = 3.2396\n",
      "Epoch 12, Batch 160, Loss = 3.2175\n",
      "Epoch 12, Batch 170, Loss = 3.2341\n",
      "Epoch 12, Batch 180, Loss = 3.0477\n",
      "Epoch 12, Batch 190, Loss = 3.2421\n",
      "Epoch 12, Batch 200, Loss = 3.0957\n",
      "Epoch 12, Batch 210, Loss = 3.0204\n",
      "Epoch 12, Batch 220, Loss = 3.1178\n",
      "Epoch 12, Batch 230, Loss = 3.1684\n",
      "Epoch 12, Batch 240, Loss = 3.1688\n",
      "Epoch 12, Batch 250, Loss = 3.0092\n",
      "Epoch 12, Batch 260, Loss = 3.1181\n",
      "Epoch 12, Batch 270, Loss = 3.1901\n",
      "Epoch 12, Batch 280, Loss = 3.2878\n",
      "Epoch 12, Batch 290, Loss = 3.1200\n",
      "Epoch 12, Batch 300, Loss = 3.1172\n",
      "Epoch 12, Batch 310, Loss = 3.2756\n",
      "Epoch 12, Batch 320, Loss = 3.1366\n",
      "Epoch 12, Batch 330, Loss = 3.0248\n",
      "Epoch 12, Batch 340, Loss = 3.2028\n",
      "Epoch 12, Batch 350, Loss = 3.1831\n",
      "Epoch 12, Batch 360, Loss = 3.2205\n",
      "Epoch 12, Batch 370, Loss = 3.1955\n",
      "Epoch 12, Batch 380, Loss = 3.2843\n",
      "Epoch 12, Batch 390, Loss = 3.4085\n",
      "Epoch 12, Batch 400, Loss = 3.1363\n",
      "Epoch 12, Batch 410, Loss = 3.1446\n",
      "Epoch 12, Batch 420, Loss = 3.2673\n",
      "Epoch 12, Batch 430, Loss = 3.1241\n",
      "Epoch 12, Batch 440, Loss = 3.1211\n",
      "Epoch 12, Batch 450, Loss = 3.1057\n",
      "Epoch 12, Batch 460, Loss = 3.2199\n",
      "Epoch 12, Batch 470, Loss = 3.0081\n",
      "Epoch 12, Batch 480, Loss = 3.1730\n",
      "Epoch 12, Batch 490, Loss = 3.1926\n",
      "Epoch 12, Batch 500, Loss = 3.1072\n",
      "Epoch 12, Batch 510, Loss = 3.0399\n",
      "Epoch 12, Batch 520, Loss = 3.0366\n",
      "Epoch 12, Batch 530, Loss = 3.1682\n",
      "Epoch 12, Batch 540, Loss = 3.1181\n",
      "Epoch 12, Batch 550, Loss = 3.2171\n",
      "Epoch 12, Batch 560, Loss = 3.2291\n",
      "Epoch 12, Batch 570, Loss = 3.1199\n",
      "Epoch 12, Batch 580, Loss = 3.1217\n",
      "Epoch 12, Batch 590, Loss = 3.1784\n",
      "Epoch 12, Batch 600, Loss = 3.2380\n",
      "Epoch 12, Batch 610, Loss = 3.0475\n",
      "Epoch 12, Batch 620, Loss = 3.1384\n",
      "Epoch 12, Batch 630, Loss = 3.0183\n",
      "Epoch 12, Batch 640, Loss = 3.0207\n",
      "Epoch 12, Batch 650, Loss = 3.2313\n",
      "Epoch 12, Batch 660, Loss = 3.0804\n",
      "Epoch 12, Batch 670, Loss = 3.0796\n",
      "Epoch 12, Batch 680, Loss = 3.1556\n",
      "Epoch 12, Batch 690, Loss = 3.2766\n",
      "Epoch 12, Batch 700, Loss = 3.1230\n",
      "Epoch 12, Batch 710, Loss = 3.2426\n",
      "Epoch 12, Batch 720, Loss = 3.2049\n",
      "Epoch 12, Batch 730, Loss = 3.1746\n",
      "Epoch 12, Batch 740, Loss = 3.1225\n",
      "Epoch 12, Batch 750, Loss = 3.3033\n",
      "Epoch 12, Batch 760, Loss = 3.0518\n",
      "Epoch 12, Batch 770, Loss = 3.1945\n",
      "Epoch 12, Batch 780, Loss = 3.1436\n",
      "Epoch 12, Batch 790, Loss = 3.1607\n",
      "Epoch 12, Batch 800, Loss = 3.2735\n",
      "Epoch 12, Batch 810, Loss = 3.2096\n",
      "Epoch 12, Batch 820, Loss = 3.3126\n",
      "Epoch 12, Batch 830, Loss = 3.1695\n",
      "Epoch 12, Batch 840, Loss = 3.1536\n",
      "Epoch 12, Batch 850, Loss = 3.1793\n",
      "Epoch 12, Batch 860, Loss = 3.1937\n",
      "Epoch 12, Batch 870, Loss = 3.3242\n",
      "Epoch 12, Batch 880, Loss = 3.1479\n",
      "Epoch 12, Batch 890, Loss = 3.1051\n",
      "Epoch 12, Batch 900, Loss = 3.0863\n",
      "Epoch 12, Batch 910, Loss = 3.2085\n",
      "Epoch 12, Batch 920, Loss = 3.1176\n",
      "Epoch 12, Batch 930, Loss = 3.0713\n",
      "Epoch 12, Batch 940, Loss = 3.1414\n",
      "Epoch 12, Batch 950, Loss = 3.1171\n",
      "Epoch 12, Batch 960, Loss = 3.2666\n",
      "Epoch 12, Batch 970, Loss = 3.1984\n",
      "Epoch 12, Batch 980, Loss = 3.1024\n",
      "Epoch 12, Batch 990, Loss = 3.0411\n",
      "Epoch 12, Batch 1000, Loss = 3.1880\n",
      "Epoch 12, Batch 1010, Loss = 3.1938\n",
      "Epoch 12, Batch 1020, Loss = 3.2635\n",
      "Epoch 12, Batch 1030, Loss = 3.1665\n",
      "Epoch 12, Batch 1040, Loss = 3.2755\n",
      "Epoch 12, Batch 1050, Loss = 3.1823\n",
      "Epoch 12, Batch 1060, Loss = 3.0997\n",
      "Epoch 12, Batch 1070, Loss = 3.3056\n",
      "Epoch 12, Batch 1080, Loss = 3.1926\n",
      "Epoch 12, Batch 1090, Loss = 3.0925\n",
      "Epoch 12, Batch 1100, Loss = 3.1450\n",
      "Epoch 12, Batch 1110, Loss = 3.3626\n",
      "Epoch 12, Batch 1120, Loss = 3.0452\n",
      "Epoch 12, Average Loss = 3.1697\n",
      "Epoch 13, Batch 0, Loss = 3.1893\n",
      "Epoch 13, Batch 10, Loss = 3.0838\n",
      "Epoch 13, Batch 20, Loss = 3.1942\n",
      "Epoch 13, Batch 30, Loss = 3.1903\n",
      "Epoch 13, Batch 40, Loss = 3.2085\n",
      "Epoch 13, Batch 50, Loss = 3.2570\n",
      "Epoch 13, Batch 60, Loss = 3.0765\n",
      "Epoch 13, Batch 70, Loss = 3.3023\n",
      "Epoch 13, Batch 80, Loss = 3.3840\n",
      "Epoch 13, Batch 90, Loss = 3.0070\n",
      "Epoch 13, Batch 100, Loss = 3.1972\n",
      "Epoch 13, Batch 110, Loss = 3.1083\n",
      "Epoch 13, Batch 120, Loss = 3.0633\n",
      "Epoch 13, Batch 130, Loss = 3.0613\n",
      "Epoch 13, Batch 140, Loss = 3.2203\n",
      "Epoch 13, Batch 150, Loss = 3.1049\n",
      "Epoch 13, Batch 160, Loss = 3.3291\n",
      "Epoch 13, Batch 170, Loss = 3.1201\n",
      "Epoch 13, Batch 180, Loss = 3.1545\n",
      "Epoch 13, Batch 190, Loss = 3.2505\n",
      "Epoch 13, Batch 200, Loss = 3.2420\n",
      "Epoch 13, Batch 210, Loss = 3.0481\n",
      "Epoch 13, Batch 220, Loss = 3.1424\n",
      "Epoch 13, Batch 230, Loss = 3.1220\n",
      "Epoch 13, Batch 240, Loss = 3.0278\n",
      "Epoch 13, Batch 250, Loss = 3.2214\n",
      "Epoch 13, Batch 260, Loss = 3.1127\n",
      "Epoch 13, Batch 270, Loss = 2.9298\n",
      "Epoch 13, Batch 280, Loss = 3.0363\n",
      "Epoch 13, Batch 290, Loss = 3.2340\n",
      "Epoch 13, Batch 300, Loss = 3.1260\n",
      "Epoch 13, Batch 310, Loss = 3.2164\n",
      "Epoch 13, Batch 320, Loss = 3.1544\n",
      "Epoch 13, Batch 330, Loss = 3.1729\n",
      "Epoch 13, Batch 340, Loss = 3.2192\n",
      "Epoch 13, Batch 350, Loss = 3.2959\n",
      "Epoch 13, Batch 360, Loss = 3.3509\n",
      "Epoch 13, Batch 370, Loss = 3.2051\n",
      "Epoch 13, Batch 380, Loss = 3.0288\n",
      "Epoch 13, Batch 390, Loss = 3.1607\n",
      "Epoch 13, Batch 400, Loss = 3.1557\n",
      "Epoch 13, Batch 410, Loss = 3.1965\n",
      "Epoch 13, Batch 420, Loss = 3.2412\n",
      "Epoch 13, Batch 430, Loss = 3.2979\n",
      "Epoch 13, Batch 440, Loss = 3.0960\n",
      "Epoch 13, Batch 450, Loss = 3.2548\n",
      "Epoch 13, Batch 460, Loss = 3.0608\n",
      "Epoch 13, Batch 470, Loss = 3.2483\n",
      "Epoch 13, Batch 480, Loss = 3.1795\n",
      "Epoch 13, Batch 490, Loss = 3.2105\n",
      "Epoch 13, Batch 500, Loss = 3.1328\n",
      "Epoch 13, Batch 510, Loss = 3.2302\n",
      "Epoch 13, Batch 520, Loss = 3.2652\n",
      "Epoch 13, Batch 530, Loss = 3.1142\n",
      "Epoch 13, Batch 540, Loss = 3.2757\n",
      "Epoch 13, Batch 550, Loss = 3.0473\n",
      "Epoch 13, Batch 560, Loss = 3.1460\n",
      "Epoch 13, Batch 570, Loss = 3.2146\n",
      "Epoch 13, Batch 580, Loss = 3.1937\n",
      "Epoch 13, Batch 590, Loss = 3.1195\n",
      "Epoch 13, Batch 600, Loss = 3.2150\n",
      "Epoch 13, Batch 610, Loss = 3.0218\n",
      "Epoch 13, Batch 620, Loss = 3.2360\n",
      "Epoch 13, Batch 630, Loss = 3.2275\n",
      "Epoch 13, Batch 640, Loss = 3.1905\n",
      "Epoch 13, Batch 650, Loss = 3.2582\n",
      "Epoch 13, Batch 660, Loss = 3.1976\n",
      "Epoch 13, Batch 670, Loss = 3.0814\n",
      "Epoch 13, Batch 680, Loss = 3.1102\n",
      "Epoch 13, Batch 690, Loss = 3.1666\n",
      "Epoch 13, Batch 700, Loss = 3.1980\n",
      "Epoch 13, Batch 710, Loss = 3.1665\n",
      "Epoch 13, Batch 720, Loss = 3.0098\n",
      "Epoch 13, Batch 730, Loss = 3.1375\n",
      "Epoch 13, Batch 740, Loss = 3.0533\n",
      "Epoch 13, Batch 750, Loss = 3.2208\n",
      "Epoch 13, Batch 760, Loss = 3.2587\n",
      "Epoch 13, Batch 770, Loss = 3.2026\n",
      "Epoch 13, Batch 780, Loss = 3.1159\n",
      "Epoch 13, Batch 790, Loss = 3.2028\n",
      "Epoch 13, Batch 800, Loss = 3.1164\n",
      "Epoch 13, Batch 810, Loss = 3.0933\n",
      "Epoch 13, Batch 820, Loss = 3.0899\n",
      "Epoch 13, Batch 830, Loss = 3.1711\n",
      "Epoch 13, Batch 840, Loss = 3.0718\n",
      "Epoch 13, Batch 850, Loss = 3.2528\n",
      "Epoch 13, Batch 860, Loss = 3.1553\n",
      "Epoch 13, Batch 870, Loss = 3.1101\n",
      "Epoch 13, Batch 880, Loss = 3.1989\n",
      "Epoch 13, Batch 890, Loss = 3.0847\n",
      "Epoch 13, Batch 900, Loss = 3.2856\n",
      "Epoch 13, Batch 910, Loss = 3.1205\n",
      "Epoch 13, Batch 920, Loss = 3.0673\n",
      "Epoch 13, Batch 930, Loss = 3.1734\n",
      "Epoch 13, Batch 940, Loss = 3.2775\n",
      "Epoch 13, Batch 950, Loss = 3.1176\n",
      "Epoch 13, Batch 960, Loss = 3.2817\n",
      "Epoch 13, Batch 970, Loss = 3.0800\n",
      "Epoch 13, Batch 980, Loss = 3.1429\n",
      "Epoch 13, Batch 990, Loss = 3.2548\n",
      "Epoch 13, Batch 1000, Loss = 3.2031\n",
      "Epoch 13, Batch 1010, Loss = 3.2328\n",
      "Epoch 13, Batch 1020, Loss = 3.2254\n",
      "Epoch 13, Batch 1030, Loss = 3.1995\n",
      "Epoch 13, Batch 1040, Loss = 3.2490\n",
      "Epoch 13, Batch 1050, Loss = 3.1506\n",
      "Epoch 13, Batch 1060, Loss = 3.1908\n",
      "Epoch 13, Batch 1070, Loss = 3.2582\n",
      "Epoch 13, Batch 1080, Loss = 3.1043\n",
      "Epoch 13, Batch 1090, Loss = 3.1983\n",
      "Epoch 13, Batch 1100, Loss = 3.1863\n",
      "Epoch 13, Batch 1110, Loss = 3.1361\n",
      "Epoch 13, Batch 1120, Loss = 3.1060\n",
      "Epoch 13, Average Loss = 3.1651\n",
      "Epoch 14, Batch 0, Loss = 3.1851\n",
      "Epoch 14, Batch 10, Loss = 2.8764\n",
      "Epoch 14, Batch 20, Loss = 3.0565\n",
      "Epoch 14, Batch 30, Loss = 3.1046\n",
      "Epoch 14, Batch 40, Loss = 3.0985\n",
      "Epoch 14, Batch 50, Loss = 3.2894\n",
      "Epoch 14, Batch 60, Loss = 2.9911\n",
      "Epoch 14, Batch 70, Loss = 3.1762\n",
      "Epoch 14, Batch 80, Loss = 3.1406\n",
      "Epoch 14, Batch 90, Loss = 3.3014\n",
      "Epoch 14, Batch 100, Loss = 3.1320\n",
      "Epoch 14, Batch 110, Loss = 3.1279\n",
      "Epoch 14, Batch 120, Loss = 3.3071\n",
      "Epoch 14, Batch 130, Loss = 3.1041\n",
      "Epoch 14, Batch 140, Loss = 3.2323\n",
      "Epoch 14, Batch 150, Loss = 3.1960\n",
      "Epoch 14, Batch 160, Loss = 3.1917\n",
      "Epoch 14, Batch 170, Loss = 3.1957\n",
      "Epoch 14, Batch 180, Loss = 3.1981\n",
      "Epoch 14, Batch 190, Loss = 3.2279\n",
      "Epoch 14, Batch 200, Loss = 3.1462\n",
      "Epoch 14, Batch 210, Loss = 3.3269\n",
      "Epoch 14, Batch 220, Loss = 3.1820\n",
      "Epoch 14, Batch 230, Loss = 3.1700\n",
      "Epoch 14, Batch 240, Loss = 3.2717\n",
      "Epoch 14, Batch 250, Loss = 3.1145\n",
      "Epoch 14, Batch 260, Loss = 3.2172\n",
      "Epoch 14, Batch 270, Loss = 3.2481\n",
      "Epoch 14, Batch 280, Loss = 3.0970\n",
      "Epoch 14, Batch 290, Loss = 3.2284\n",
      "Epoch 14, Batch 300, Loss = 3.1995\n",
      "Epoch 14, Batch 310, Loss = 3.1884\n",
      "Epoch 14, Batch 320, Loss = 3.1015\n",
      "Epoch 14, Batch 330, Loss = 3.1572\n",
      "Epoch 14, Batch 340, Loss = 3.1341\n",
      "Epoch 14, Batch 350, Loss = 3.1658\n",
      "Epoch 14, Batch 360, Loss = 3.1819\n",
      "Epoch 14, Batch 370, Loss = 3.3598\n",
      "Epoch 14, Batch 380, Loss = 3.2136\n",
      "Epoch 14, Batch 390, Loss = 3.2473\n",
      "Epoch 14, Batch 400, Loss = 3.0071\n",
      "Epoch 14, Batch 410, Loss = 3.2207\n",
      "Epoch 14, Batch 420, Loss = 3.2262\n",
      "Epoch 14, Batch 430, Loss = 3.0776\n",
      "Epoch 14, Batch 440, Loss = 3.0032\n",
      "Epoch 14, Batch 450, Loss = 3.1312\n",
      "Epoch 14, Batch 460, Loss = 3.0395\n",
      "Epoch 14, Batch 470, Loss = 3.1068\n",
      "Epoch 14, Batch 480, Loss = 3.1405\n",
      "Epoch 14, Batch 490, Loss = 3.2396\n",
      "Epoch 14, Batch 500, Loss = 3.1660\n",
      "Epoch 14, Batch 510, Loss = 3.1693\n",
      "Epoch 14, Batch 520, Loss = 3.0634\n",
      "Epoch 14, Batch 530, Loss = 3.1130\n",
      "Epoch 14, Batch 540, Loss = 3.2076\n",
      "Epoch 14, Batch 550, Loss = 3.2888\n",
      "Epoch 14, Batch 560, Loss = 3.0658\n",
      "Epoch 14, Batch 570, Loss = 3.2904\n",
      "Epoch 14, Batch 580, Loss = 3.2986\n",
      "Epoch 14, Batch 590, Loss = 3.1306\n",
      "Epoch 14, Batch 600, Loss = 3.2653\n",
      "Epoch 14, Batch 610, Loss = 3.0325\n",
      "Epoch 14, Batch 620, Loss = 3.2619\n",
      "Epoch 14, Batch 630, Loss = 3.1889\n",
      "Epoch 14, Batch 640, Loss = 3.1148\n",
      "Epoch 14, Batch 650, Loss = 3.2184\n",
      "Epoch 14, Batch 660, Loss = 3.1732\n",
      "Epoch 14, Batch 670, Loss = 3.0862\n",
      "Epoch 14, Batch 680, Loss = 3.1659\n",
      "Epoch 14, Batch 690, Loss = 3.1697\n",
      "Epoch 14, Batch 700, Loss = 3.1638\n",
      "Epoch 14, Batch 710, Loss = 3.0916\n",
      "Epoch 14, Batch 720, Loss = 3.2897\n",
      "Epoch 14, Batch 730, Loss = 3.1600\n",
      "Epoch 14, Batch 740, Loss = 3.2779\n",
      "Epoch 14, Batch 750, Loss = 3.1252\n",
      "Epoch 14, Batch 760, Loss = 3.2105\n",
      "Epoch 14, Batch 770, Loss = 3.2741\n",
      "Epoch 14, Batch 780, Loss = 3.1053\n",
      "Epoch 14, Batch 790, Loss = 3.3548\n",
      "Epoch 14, Batch 800, Loss = 3.2188\n",
      "Epoch 14, Batch 810, Loss = 3.2157\n",
      "Epoch 14, Batch 820, Loss = 3.2325\n",
      "Epoch 14, Batch 830, Loss = 3.2721\n",
      "Epoch 14, Batch 840, Loss = 3.0966\n",
      "Epoch 14, Batch 850, Loss = 3.1782\n",
      "Epoch 14, Batch 860, Loss = 3.1855\n",
      "Epoch 14, Batch 870, Loss = 3.0388\n",
      "Epoch 14, Batch 880, Loss = 3.3135\n",
      "Epoch 14, Batch 890, Loss = 3.3118\n",
      "Epoch 14, Batch 900, Loss = 3.1069\n",
      "Epoch 14, Batch 910, Loss = 3.2293\n",
      "Epoch 14, Batch 920, Loss = 3.2596\n",
      "Epoch 14, Batch 930, Loss = 3.0092\n",
      "Epoch 14, Batch 940, Loss = 3.3124\n",
      "Epoch 14, Batch 950, Loss = 3.0143\n",
      "Epoch 14, Batch 960, Loss = 3.1135\n",
      "Epoch 14, Batch 970, Loss = 3.0335\n",
      "Epoch 14, Batch 980, Loss = 3.1110\n",
      "Epoch 14, Batch 990, Loss = 3.2159\n",
      "Epoch 14, Batch 1000, Loss = 3.1980\n",
      "Epoch 14, Batch 1010, Loss = 3.0907\n",
      "Epoch 14, Batch 1020, Loss = 3.0631\n",
      "Epoch 14, Batch 1030, Loss = 3.1959\n",
      "Epoch 14, Batch 1040, Loss = 3.2100\n",
      "Epoch 14, Batch 1050, Loss = 3.1695\n",
      "Epoch 14, Batch 1060, Loss = 3.1510\n",
      "Epoch 14, Batch 1070, Loss = 3.0250\n",
      "Epoch 14, Batch 1080, Loss = 3.1302\n",
      "Epoch 14, Batch 1090, Loss = 3.0970\n",
      "Epoch 14, Batch 1100, Loss = 3.1152\n",
      "Epoch 14, Batch 1110, Loss = 3.2325\n",
      "Epoch 14, Batch 1120, Loss = 3.1662\n",
      "Epoch 14, Average Loss = 3.1631\n",
      "Epoch 15, Batch 0, Loss = 2.9865\n",
      "Epoch 15, Batch 10, Loss = 3.0949\n",
      "Epoch 15, Batch 20, Loss = 3.0893\n",
      "Epoch 15, Batch 30, Loss = 3.0787\n",
      "Epoch 15, Batch 40, Loss = 2.9782\n",
      "Epoch 15, Batch 50, Loss = 3.0173\n",
      "Epoch 15, Batch 60, Loss = 3.2001\n",
      "Epoch 15, Batch 70, Loss = 3.1595\n",
      "Epoch 15, Batch 80, Loss = 3.1456\n",
      "Epoch 15, Batch 90, Loss = 3.0692\n",
      "Epoch 15, Batch 100, Loss = 2.9746\n",
      "Epoch 15, Batch 110, Loss = 3.2259\n",
      "Epoch 15, Batch 120, Loss = 3.0262\n",
      "Epoch 15, Batch 130, Loss = 3.1766\n",
      "Epoch 15, Batch 140, Loss = 3.0976\n",
      "Epoch 15, Batch 150, Loss = 3.4241\n",
      "Epoch 15, Batch 160, Loss = 3.1210\n",
      "Epoch 15, Batch 170, Loss = 3.1396\n",
      "Epoch 15, Batch 180, Loss = 3.0914\n",
      "Epoch 15, Batch 190, Loss = 3.1010\n",
      "Epoch 15, Batch 200, Loss = 3.2486\n",
      "Epoch 15, Batch 210, Loss = 3.1446\n",
      "Epoch 15, Batch 220, Loss = 3.1016\n",
      "Epoch 15, Batch 230, Loss = 3.2078\n",
      "Epoch 15, Batch 240, Loss = 3.2609\n",
      "Epoch 15, Batch 250, Loss = 3.2471\n",
      "Epoch 15, Batch 260, Loss = 3.0972\n",
      "Epoch 15, Batch 270, Loss = 3.0504\n",
      "Epoch 15, Batch 280, Loss = 3.0913\n",
      "Epoch 15, Batch 290, Loss = 3.1588\n",
      "Epoch 15, Batch 300, Loss = 3.0890\n",
      "Epoch 15, Batch 310, Loss = 3.1436\n",
      "Epoch 15, Batch 320, Loss = 3.2183\n",
      "Epoch 15, Batch 330, Loss = 3.2776\n",
      "Epoch 15, Batch 340, Loss = 3.0046\n",
      "Epoch 15, Batch 350, Loss = 3.2077\n",
      "Epoch 15, Batch 360, Loss = 3.0497\n",
      "Epoch 15, Batch 370, Loss = 3.0731\n",
      "Epoch 15, Batch 380, Loss = 3.0824\n",
      "Epoch 15, Batch 390, Loss = 3.1816\n",
      "Epoch 15, Batch 400, Loss = 3.0283\n",
      "Epoch 15, Batch 410, Loss = 3.0871\n",
      "Epoch 15, Batch 420, Loss = 3.2166\n",
      "Epoch 15, Batch 430, Loss = 3.1335\n",
      "Epoch 15, Batch 440, Loss = 3.2472\n",
      "Epoch 15, Batch 450, Loss = 3.1390\n",
      "Epoch 15, Batch 460, Loss = 3.2232\n",
      "Epoch 15, Batch 470, Loss = 3.2023\n",
      "Epoch 15, Batch 480, Loss = 3.2939\n",
      "Epoch 15, Batch 490, Loss = 3.1663\n",
      "Epoch 15, Batch 500, Loss = 3.1472\n",
      "Epoch 15, Batch 510, Loss = 3.1749\n",
      "Epoch 15, Batch 520, Loss = 3.1106\n",
      "Epoch 15, Batch 530, Loss = 3.3089\n",
      "Epoch 15, Batch 540, Loss = 3.1084\n",
      "Epoch 15, Batch 550, Loss = 3.2532\n",
      "Epoch 15, Batch 560, Loss = 3.1368\n",
      "Epoch 15, Batch 570, Loss = 3.2614\n",
      "Epoch 15, Batch 580, Loss = 3.0872\n",
      "Epoch 15, Batch 590, Loss = 3.2024\n",
      "Epoch 15, Batch 600, Loss = 3.0283\n",
      "Epoch 15, Batch 610, Loss = 2.9703\n",
      "Epoch 15, Batch 620, Loss = 3.2126\n",
      "Epoch 15, Batch 630, Loss = 3.0894\n",
      "Epoch 15, Batch 640, Loss = 3.0688\n",
      "Epoch 15, Batch 650, Loss = 3.1033\n",
      "Epoch 15, Batch 660, Loss = 3.1180\n",
      "Epoch 15, Batch 670, Loss = 3.0739\n",
      "Epoch 15, Batch 680, Loss = 3.0655\n",
      "Epoch 15, Batch 690, Loss = 3.2439\n",
      "Epoch 15, Batch 700, Loss = 3.1608\n",
      "Epoch 15, Batch 710, Loss = 3.3852\n",
      "Epoch 15, Batch 720, Loss = 3.1643\n",
      "Epoch 15, Batch 730, Loss = 3.2053\n",
      "Epoch 15, Batch 740, Loss = 3.0749\n",
      "Epoch 15, Batch 750, Loss = 3.4169\n",
      "Epoch 15, Batch 760, Loss = 2.9860\n",
      "Epoch 15, Batch 770, Loss = 3.1362\n",
      "Epoch 15, Batch 780, Loss = 3.4703\n",
      "Epoch 15, Batch 790, Loss = 3.1456\n",
      "Epoch 15, Batch 800, Loss = 3.0700\n",
      "Epoch 15, Batch 810, Loss = 3.2102\n",
      "Epoch 15, Batch 820, Loss = 3.1047\n",
      "Epoch 15, Batch 830, Loss = 3.2926\n",
      "Epoch 15, Batch 840, Loss = 3.1317\n",
      "Epoch 15, Batch 850, Loss = 3.1024\n",
      "Epoch 15, Batch 860, Loss = 3.1820\n",
      "Epoch 15, Batch 870, Loss = 3.1458\n",
      "Epoch 15, Batch 880, Loss = 3.0945\n",
      "Epoch 15, Batch 890, Loss = 3.3352\n",
      "Epoch 15, Batch 900, Loss = 3.2744\n",
      "Epoch 15, Batch 910, Loss = 3.0571\n",
      "Epoch 15, Batch 920, Loss = 3.0548\n",
      "Epoch 15, Batch 930, Loss = 3.2554\n",
      "Epoch 15, Batch 940, Loss = 3.1449\n",
      "Epoch 15, Batch 950, Loss = 3.2872\n",
      "Epoch 15, Batch 960, Loss = 3.2539\n",
      "Epoch 15, Batch 970, Loss = 3.2797\n",
      "Epoch 15, Batch 980, Loss = 3.1507\n",
      "Epoch 15, Batch 990, Loss = 3.1259\n",
      "Epoch 15, Batch 1000, Loss = 3.1749\n",
      "Epoch 15, Batch 1010, Loss = 3.1865\n",
      "Epoch 15, Batch 1020, Loss = 3.3119\n",
      "Epoch 15, Batch 1030, Loss = 3.3483\n",
      "Epoch 15, Batch 1040, Loss = 3.1369\n",
      "Epoch 15, Batch 1050, Loss = 3.1831\n",
      "Epoch 15, Batch 1060, Loss = 3.1033\n",
      "Epoch 15, Batch 1070, Loss = 3.1662\n",
      "Epoch 15, Batch 1080, Loss = 3.1826\n",
      "Epoch 15, Batch 1090, Loss = 3.1359\n",
      "Epoch 15, Batch 1100, Loss = 3.2469\n",
      "Epoch 15, Batch 1110, Loss = 3.0995\n",
      "Epoch 15, Batch 1120, Loss = 3.2329\n",
      "Epoch 15, Average Loss = 3.1593\n",
      "Epoch 16, Batch 0, Loss = 3.1253\n",
      "Epoch 16, Batch 10, Loss = 3.1341\n",
      "Epoch 16, Batch 20, Loss = 3.1620\n",
      "Epoch 16, Batch 30, Loss = 3.0183\n",
      "Epoch 16, Batch 40, Loss = 3.2555\n",
      "Epoch 16, Batch 50, Loss = 3.0450\n",
      "Epoch 16, Batch 60, Loss = 3.0481\n",
      "Epoch 16, Batch 70, Loss = 3.2249\n",
      "Epoch 16, Batch 80, Loss = 3.1332\n",
      "Epoch 16, Batch 90, Loss = 3.2667\n",
      "Epoch 16, Batch 100, Loss = 3.0750\n",
      "Epoch 16, Batch 110, Loss = 3.1575\n",
      "Epoch 16, Batch 120, Loss = 3.2227\n",
      "Epoch 16, Batch 130, Loss = 3.0277\n",
      "Epoch 16, Batch 140, Loss = 3.1103\n",
      "Epoch 16, Batch 150, Loss = 3.1161\n",
      "Epoch 16, Batch 160, Loss = 3.2224\n",
      "Epoch 16, Batch 170, Loss = 3.1424\n",
      "Epoch 16, Batch 180, Loss = 3.2190\n",
      "Epoch 16, Batch 190, Loss = 3.2010\n",
      "Epoch 16, Batch 200, Loss = 3.2294\n",
      "Epoch 16, Batch 210, Loss = 3.1461\n",
      "Epoch 16, Batch 220, Loss = 3.1413\n",
      "Epoch 16, Batch 230, Loss = 3.1014\n",
      "Epoch 16, Batch 240, Loss = 3.0828\n",
      "Epoch 16, Batch 250, Loss = 3.2628\n",
      "Epoch 16, Batch 260, Loss = 3.0613\n",
      "Epoch 16, Batch 270, Loss = 3.1723\n",
      "Epoch 16, Batch 280, Loss = 3.1574\n",
      "Epoch 16, Batch 290, Loss = 3.1770\n",
      "Epoch 16, Batch 300, Loss = 3.3805\n",
      "Epoch 16, Batch 310, Loss = 3.2056\n",
      "Epoch 16, Batch 320, Loss = 3.1966\n",
      "Epoch 16, Batch 330, Loss = 3.1534\n",
      "Epoch 16, Batch 340, Loss = 3.0182\n",
      "Epoch 16, Batch 350, Loss = 3.1654\n",
      "Epoch 16, Batch 360, Loss = 3.1381\n",
      "Epoch 16, Batch 370, Loss = 3.1980\n",
      "Epoch 16, Batch 380, Loss = 3.1831\n",
      "Epoch 16, Batch 390, Loss = 3.3102\n",
      "Epoch 16, Batch 400, Loss = 3.1268\n",
      "Epoch 16, Batch 410, Loss = 3.0468\n",
      "Epoch 16, Batch 420, Loss = 3.1245\n",
      "Epoch 16, Batch 430, Loss = 3.1676\n",
      "Epoch 16, Batch 440, Loss = 2.9429\n",
      "Epoch 16, Batch 450, Loss = 3.2184\n",
      "Epoch 16, Batch 460, Loss = 3.2249\n",
      "Epoch 16, Batch 470, Loss = 3.0939\n",
      "Epoch 16, Batch 480, Loss = 3.2793\n",
      "Epoch 16, Batch 490, Loss = 3.1408\n",
      "Epoch 16, Batch 500, Loss = 3.1768\n",
      "Epoch 16, Batch 510, Loss = 3.2966\n",
      "Epoch 16, Batch 520, Loss = 3.1894\n",
      "Epoch 16, Batch 530, Loss = 3.0910\n",
      "Epoch 16, Batch 540, Loss = 3.3280\n",
      "Epoch 16, Batch 550, Loss = 3.2121\n",
      "Epoch 16, Batch 560, Loss = 3.2223\n",
      "Epoch 16, Batch 570, Loss = 3.2950\n",
      "Epoch 16, Batch 580, Loss = 3.1404\n",
      "Epoch 16, Batch 590, Loss = 3.0254\n",
      "Epoch 16, Batch 600, Loss = 3.2631\n",
      "Epoch 16, Batch 610, Loss = 3.0485\n",
      "Epoch 16, Batch 620, Loss = 3.1232\n",
      "Epoch 16, Batch 630, Loss = 3.1824\n",
      "Epoch 16, Batch 640, Loss = 3.2311\n",
      "Epoch 16, Batch 650, Loss = 3.0858\n",
      "Epoch 16, Batch 660, Loss = 3.1872\n",
      "Epoch 16, Batch 670, Loss = 3.1747\n",
      "Epoch 16, Batch 680, Loss = 3.1788\n",
      "Epoch 16, Batch 690, Loss = 3.1467\n",
      "Epoch 16, Batch 700, Loss = 3.2073\n",
      "Epoch 16, Batch 710, Loss = 3.1983\n",
      "Epoch 16, Batch 720, Loss = 3.1486\n",
      "Epoch 16, Batch 730, Loss = 3.2480\n",
      "Epoch 16, Batch 740, Loss = 2.9517\n",
      "Epoch 16, Batch 750, Loss = 3.1893\n",
      "Epoch 16, Batch 760, Loss = 3.2575\n",
      "Epoch 16, Batch 770, Loss = 3.2027\n",
      "Epoch 16, Batch 780, Loss = 3.2559\n",
      "Epoch 16, Batch 790, Loss = 3.0493\n",
      "Epoch 16, Batch 800, Loss = 3.1010\n",
      "Epoch 16, Batch 810, Loss = 3.3564\n",
      "Epoch 16, Batch 820, Loss = 3.2120\n",
      "Epoch 16, Batch 830, Loss = 3.0750\n",
      "Epoch 16, Batch 840, Loss = 3.1180\n",
      "Epoch 16, Batch 850, Loss = 3.1254\n",
      "Epoch 16, Batch 860, Loss = 3.1545\n",
      "Epoch 16, Batch 870, Loss = 3.1042\n",
      "Epoch 16, Batch 880, Loss = 3.2491\n",
      "Epoch 16, Batch 890, Loss = 3.2336\n",
      "Epoch 16, Batch 900, Loss = 3.0890\n",
      "Epoch 16, Batch 910, Loss = 3.0422\n",
      "Epoch 16, Batch 920, Loss = 3.0021\n",
      "Epoch 16, Batch 930, Loss = 3.0537\n",
      "Epoch 16, Batch 940, Loss = 3.0304\n",
      "Epoch 16, Batch 950, Loss = 3.2634\n",
      "Epoch 16, Batch 960, Loss = 3.1794\n",
      "Epoch 16, Batch 970, Loss = 3.1432\n",
      "Epoch 16, Batch 980, Loss = 3.1801\n",
      "Epoch 16, Batch 990, Loss = 3.1619\n",
      "Epoch 16, Batch 1000, Loss = 3.0806\n",
      "Epoch 16, Batch 1010, Loss = 3.2533\n",
      "Epoch 16, Batch 1020, Loss = 3.2889\n",
      "Epoch 16, Batch 1030, Loss = 3.1070\n",
      "Epoch 16, Batch 1040, Loss = 3.2418\n",
      "Epoch 16, Batch 1050, Loss = 3.0175\n",
      "Epoch 16, Batch 1060, Loss = 3.1041\n",
      "Epoch 16, Batch 1070, Loss = 3.2043\n",
      "Epoch 16, Batch 1080, Loss = 3.1288\n",
      "Epoch 16, Batch 1090, Loss = 3.3080\n",
      "Epoch 16, Batch 1100, Loss = 2.9822\n",
      "Epoch 16, Batch 1110, Loss = 3.0940\n",
      "Epoch 16, Batch 1120, Loss = 3.2193\n",
      "Epoch 16, Average Loss = 3.1578\n",
      "Epoch 17, Batch 0, Loss = 3.1166\n",
      "Epoch 17, Batch 10, Loss = 3.1387\n",
      "Epoch 17, Batch 20, Loss = 3.0862\n",
      "Epoch 17, Batch 30, Loss = 3.2343\n",
      "Epoch 17, Batch 40, Loss = 3.1879\n",
      "Epoch 17, Batch 50, Loss = 3.2571\n",
      "Epoch 17, Batch 60, Loss = 3.1363\n",
      "Epoch 17, Batch 70, Loss = 3.1848\n",
      "Epoch 17, Batch 80, Loss = 3.0754\n",
      "Epoch 17, Batch 90, Loss = 3.1395\n",
      "Epoch 17, Batch 100, Loss = 3.0657\n",
      "Epoch 17, Batch 110, Loss = 3.1210\n",
      "Epoch 17, Batch 120, Loss = 3.1992\n",
      "Epoch 17, Batch 130, Loss = 3.1659\n",
      "Epoch 17, Batch 140, Loss = 3.0569\n",
      "Epoch 17, Batch 150, Loss = 3.3147\n",
      "Epoch 17, Batch 160, Loss = 3.1089\n",
      "Epoch 17, Batch 170, Loss = 3.1536\n",
      "Epoch 17, Batch 180, Loss = 3.1798\n",
      "Epoch 17, Batch 190, Loss = 3.1304\n",
      "Epoch 17, Batch 200, Loss = 3.0213\n",
      "Epoch 17, Batch 210, Loss = 3.1293\n",
      "Epoch 17, Batch 220, Loss = 3.1197\n",
      "Epoch 17, Batch 230, Loss = 3.2276\n",
      "Epoch 17, Batch 240, Loss = 3.1312\n",
      "Epoch 17, Batch 250, Loss = 3.2321\n",
      "Epoch 17, Batch 260, Loss = 3.1639\n",
      "Epoch 17, Batch 270, Loss = 3.1932\n",
      "Epoch 17, Batch 280, Loss = 3.0987\n",
      "Epoch 17, Batch 290, Loss = 3.1265\n",
      "Epoch 17, Batch 300, Loss = 3.1161\n",
      "Epoch 17, Batch 310, Loss = 3.1196\n",
      "Epoch 17, Batch 320, Loss = 3.0911\n",
      "Epoch 17, Batch 330, Loss = 3.2235\n",
      "Epoch 17, Batch 340, Loss = 3.1453\n",
      "Epoch 17, Batch 350, Loss = 3.2729\n",
      "Epoch 17, Batch 360, Loss = 3.0579\n",
      "Epoch 17, Batch 370, Loss = 3.2394\n",
      "Epoch 17, Batch 380, Loss = 3.1504\n",
      "Epoch 17, Batch 390, Loss = 3.0703\n",
      "Epoch 17, Batch 400, Loss = 3.0952\n",
      "Epoch 17, Batch 410, Loss = 2.9554\n",
      "Epoch 17, Batch 420, Loss = 2.9437\n",
      "Epoch 17, Batch 430, Loss = 3.1613\n",
      "Epoch 17, Batch 440, Loss = 3.2502\n",
      "Epoch 17, Batch 450, Loss = 3.0558\n",
      "Epoch 17, Batch 460, Loss = 3.1840\n",
      "Epoch 17, Batch 470, Loss = 3.1486\n",
      "Epoch 17, Batch 480, Loss = 3.1187\n",
      "Epoch 17, Batch 490, Loss = 3.1576\n",
      "Epoch 17, Batch 500, Loss = 3.1650\n",
      "Epoch 17, Batch 510, Loss = 3.1958\n",
      "Epoch 17, Batch 520, Loss = 3.1710\n",
      "Epoch 17, Batch 530, Loss = 3.2323\n",
      "Epoch 17, Batch 540, Loss = 3.0434\n",
      "Epoch 17, Batch 550, Loss = 3.1702\n",
      "Epoch 17, Batch 560, Loss = 3.1356\n",
      "Epoch 17, Batch 570, Loss = 3.0689\n",
      "Epoch 17, Batch 580, Loss = 3.1575\n",
      "Epoch 17, Batch 590, Loss = 3.1565\n",
      "Epoch 17, Batch 600, Loss = 3.0962\n",
      "Epoch 17, Batch 610, Loss = 3.0189\n",
      "Epoch 17, Batch 620, Loss = 3.1438\n",
      "Epoch 17, Batch 630, Loss = 3.1004\n",
      "Epoch 17, Batch 640, Loss = 3.2051\n",
      "Epoch 17, Batch 650, Loss = 3.1269\n",
      "Epoch 17, Batch 660, Loss = 3.1117\n",
      "Epoch 17, Batch 670, Loss = 3.1678\n",
      "Epoch 17, Batch 680, Loss = 3.0609\n",
      "Epoch 17, Batch 690, Loss = 3.1585\n",
      "Epoch 17, Batch 700, Loss = 3.1278\n",
      "Epoch 17, Batch 710, Loss = 3.3236\n",
      "Epoch 17, Batch 720, Loss = 3.1686\n",
      "Epoch 17, Batch 730, Loss = 3.0874\n",
      "Epoch 17, Batch 740, Loss = 3.1387\n",
      "Epoch 17, Batch 750, Loss = 3.0242\n",
      "Epoch 17, Batch 760, Loss = 3.1006\n",
      "Epoch 17, Batch 770, Loss = 3.1322\n",
      "Epoch 17, Batch 780, Loss = 3.1322\n",
      "Epoch 17, Batch 790, Loss = 3.2090\n",
      "Epoch 17, Batch 800, Loss = 3.1935\n",
      "Epoch 17, Batch 810, Loss = 3.0418\n",
      "Epoch 17, Batch 820, Loss = 3.2924\n",
      "Epoch 17, Batch 830, Loss = 3.2036\n",
      "Epoch 17, Batch 840, Loss = 3.2515\n",
      "Epoch 17, Batch 850, Loss = 3.1954\n",
      "Epoch 17, Batch 860, Loss = 3.1706\n",
      "Epoch 17, Batch 870, Loss = 3.3310\n",
      "Epoch 17, Batch 880, Loss = 3.1288\n",
      "Epoch 17, Batch 890, Loss = 3.0875\n",
      "Epoch 17, Batch 900, Loss = 3.1821\n",
      "Epoch 17, Batch 910, Loss = 3.2210\n",
      "Epoch 17, Batch 920, Loss = 3.3332\n",
      "Epoch 17, Batch 930, Loss = 3.2581\n",
      "Epoch 17, Batch 940, Loss = 3.0649\n",
      "Epoch 17, Batch 950, Loss = 3.2830\n",
      "Epoch 17, Batch 960, Loss = 3.1307\n",
      "Epoch 17, Batch 970, Loss = 3.0669\n",
      "Epoch 17, Batch 980, Loss = 3.1206\n",
      "Epoch 17, Batch 990, Loss = 3.0851\n",
      "Epoch 17, Batch 1000, Loss = 3.2193\n",
      "Epoch 17, Batch 1010, Loss = 3.2705\n",
      "Epoch 17, Batch 1020, Loss = 3.1504\n",
      "Epoch 17, Batch 1030, Loss = 3.1751\n",
      "Epoch 17, Batch 1040, Loss = 3.2349\n",
      "Epoch 17, Batch 1050, Loss = 3.0814\n",
      "Epoch 17, Batch 1060, Loss = 3.1538\n",
      "Epoch 17, Batch 1070, Loss = 3.1884\n",
      "Epoch 17, Batch 1080, Loss = 3.1666\n",
      "Epoch 17, Batch 1090, Loss = 3.3066\n",
      "Epoch 17, Batch 1100, Loss = 3.0191\n",
      "Epoch 17, Batch 1110, Loss = 3.1805\n",
      "Epoch 17, Batch 1120, Loss = 3.2414\n",
      "Epoch 17, Average Loss = 3.1540\n",
      "Epoch 18, Batch 0, Loss = 3.1493\n",
      "Epoch 18, Batch 10, Loss = 3.0666\n",
      "Epoch 18, Batch 20, Loss = 3.1833\n",
      "Epoch 18, Batch 30, Loss = 3.1316\n",
      "Epoch 18, Batch 40, Loss = 3.0504\n",
      "Epoch 18, Batch 50, Loss = 3.1353\n",
      "Epoch 18, Batch 60, Loss = 3.2033\n",
      "Epoch 18, Batch 70, Loss = 3.1177\n",
      "Epoch 18, Batch 80, Loss = 3.1661\n",
      "Epoch 18, Batch 90, Loss = 3.1632\n",
      "Epoch 18, Batch 100, Loss = 3.0803\n",
      "Epoch 18, Batch 110, Loss = 3.2310\n",
      "Epoch 18, Batch 120, Loss = 3.1385\n",
      "Epoch 18, Batch 130, Loss = 3.0868\n",
      "Epoch 18, Batch 140, Loss = 3.0674\n",
      "Epoch 18, Batch 150, Loss = 3.1222\n",
      "Epoch 18, Batch 160, Loss = 3.2573\n",
      "Epoch 18, Batch 170, Loss = 3.1373\n",
      "Epoch 18, Batch 180, Loss = 3.1652\n",
      "Epoch 18, Batch 190, Loss = 3.3248\n",
      "Epoch 18, Batch 200, Loss = 3.0137\n",
      "Epoch 18, Batch 210, Loss = 3.1555\n",
      "Epoch 18, Batch 220, Loss = 3.0228\n",
      "Epoch 18, Batch 230, Loss = 3.1951\n",
      "Epoch 18, Batch 240, Loss = 3.1885\n",
      "Epoch 18, Batch 250, Loss = 3.1768\n",
      "Epoch 18, Batch 260, Loss = 3.3416\n",
      "Epoch 18, Batch 270, Loss = 3.2717\n",
      "Epoch 18, Batch 280, Loss = 3.1232\n",
      "Epoch 18, Batch 290, Loss = 3.2054\n",
      "Epoch 18, Batch 300, Loss = 3.1959\n",
      "Epoch 18, Batch 310, Loss = 3.0912\n",
      "Epoch 18, Batch 320, Loss = 3.2488\n",
      "Epoch 18, Batch 330, Loss = 3.1286\n",
      "Epoch 18, Batch 340, Loss = 3.1940\n",
      "Epoch 18, Batch 350, Loss = 3.1618\n",
      "Epoch 18, Batch 360, Loss = 3.1017\n",
      "Epoch 18, Batch 370, Loss = 3.0694\n",
      "Epoch 18, Batch 380, Loss = 3.1587\n",
      "Epoch 18, Batch 390, Loss = 2.9675\n",
      "Epoch 18, Batch 400, Loss = 3.2422\n",
      "Epoch 18, Batch 410, Loss = 3.1954\n",
      "Epoch 18, Batch 420, Loss = 3.2933\n",
      "Epoch 18, Batch 430, Loss = 3.1413\n",
      "Epoch 18, Batch 440, Loss = 3.0913\n",
      "Epoch 18, Batch 450, Loss = 3.1271\n",
      "Epoch 18, Batch 460, Loss = 3.1149\n",
      "Epoch 18, Batch 470, Loss = 3.0319\n",
      "Epoch 18, Batch 480, Loss = 3.2197\n",
      "Epoch 18, Batch 490, Loss = 3.1417\n",
      "Epoch 18, Batch 500, Loss = 3.2342\n",
      "Epoch 18, Batch 510, Loss = 3.1603\n",
      "Epoch 18, Batch 520, Loss = 3.2216\n",
      "Epoch 18, Batch 530, Loss = 3.3000\n",
      "Epoch 18, Batch 540, Loss = 3.2516\n",
      "Epoch 18, Batch 550, Loss = 3.3206\n",
      "Epoch 18, Batch 560, Loss = 3.0864\n",
      "Epoch 18, Batch 570, Loss = 2.9827\n",
      "Epoch 18, Batch 580, Loss = 3.2135\n",
      "Epoch 18, Batch 590, Loss = 2.9911\n",
      "Epoch 18, Batch 600, Loss = 3.1213\n",
      "Epoch 18, Batch 610, Loss = 3.2566\n",
      "Epoch 18, Batch 620, Loss = 3.0859\n",
      "Epoch 18, Batch 630, Loss = 3.1405\n",
      "Epoch 18, Batch 640, Loss = 3.0168\n",
      "Epoch 18, Batch 650, Loss = 3.0379\n",
      "Epoch 18, Batch 660, Loss = 3.2188\n",
      "Epoch 18, Batch 670, Loss = 3.0752\n",
      "Epoch 18, Batch 680, Loss = 3.1261\n",
      "Epoch 18, Batch 690, Loss = 3.1560\n",
      "Epoch 18, Batch 700, Loss = 3.1836\n",
      "Epoch 18, Batch 710, Loss = 3.1783\n",
      "Epoch 18, Batch 720, Loss = 3.1774\n",
      "Epoch 18, Batch 730, Loss = 3.1308\n",
      "Epoch 18, Batch 740, Loss = 3.1646\n",
      "Epoch 18, Batch 750, Loss = 3.1456\n",
      "Epoch 18, Batch 760, Loss = 3.2822\n",
      "Epoch 18, Batch 770, Loss = 3.1566\n",
      "Epoch 18, Batch 780, Loss = 3.1695\n",
      "Epoch 18, Batch 790, Loss = 3.0847\n",
      "Epoch 18, Batch 800, Loss = 3.0005\n",
      "Epoch 18, Batch 810, Loss = 3.1163\n",
      "Epoch 18, Batch 820, Loss = 2.9272\n",
      "Epoch 18, Batch 830, Loss = 3.0225\n",
      "Epoch 18, Batch 840, Loss = 3.2011\n",
      "Epoch 18, Batch 850, Loss = 3.2359\n",
      "Epoch 18, Batch 860, Loss = 3.1608\n",
      "Epoch 18, Batch 870, Loss = 3.2278\n",
      "Epoch 18, Batch 880, Loss = 3.2353\n",
      "Epoch 18, Batch 890, Loss = 3.2788\n",
      "Epoch 18, Batch 900, Loss = 3.1322\n",
      "Epoch 18, Batch 910, Loss = 3.1625\n",
      "Epoch 18, Batch 920, Loss = 3.1912\n",
      "Epoch 18, Batch 930, Loss = 3.1662\n",
      "Epoch 18, Batch 940, Loss = 3.1524\n",
      "Epoch 18, Batch 950, Loss = 3.1408\n",
      "Epoch 18, Batch 960, Loss = 3.1394\n",
      "Epoch 18, Batch 970, Loss = 3.2000\n",
      "Epoch 18, Batch 980, Loss = 3.2127\n",
      "Epoch 18, Batch 990, Loss = 3.1600\n",
      "Epoch 18, Batch 1000, Loss = 3.0746\n",
      "Epoch 18, Batch 1010, Loss = 3.0651\n",
      "Epoch 18, Batch 1020, Loss = 3.2173\n",
      "Epoch 18, Batch 1030, Loss = 3.1727\n",
      "Epoch 18, Batch 1040, Loss = 3.3120\n",
      "Epoch 18, Batch 1050, Loss = 3.0796\n",
      "Epoch 18, Batch 1060, Loss = 3.2928\n",
      "Epoch 18, Batch 1070, Loss = 3.1132\n",
      "Epoch 18, Batch 1080, Loss = 3.2489\n",
      "Epoch 18, Batch 1090, Loss = 3.1110\n",
      "Epoch 18, Batch 1100, Loss = 2.9860\n",
      "Epoch 18, Batch 1110, Loss = 3.0797\n",
      "Epoch 18, Batch 1120, Loss = 3.2101\n",
      "Epoch 18, Average Loss = 3.1519\n",
      "Epoch 19, Batch 0, Loss = 2.9658\n",
      "Epoch 19, Batch 10, Loss = 3.1674\n",
      "Epoch 19, Batch 20, Loss = 3.1107\n",
      "Epoch 19, Batch 30, Loss = 3.1982\n",
      "Epoch 19, Batch 40, Loss = 3.1774\n",
      "Epoch 19, Batch 50, Loss = 2.8766\n",
      "Epoch 19, Batch 60, Loss = 3.0661\n",
      "Epoch 19, Batch 70, Loss = 3.0504\n",
      "Epoch 19, Batch 80, Loss = 2.9211\n",
      "Epoch 19, Batch 90, Loss = 3.1509\n",
      "Epoch 19, Batch 100, Loss = 3.2907\n",
      "Epoch 19, Batch 110, Loss = 3.2861\n",
      "Epoch 19, Batch 120, Loss = 3.1952\n",
      "Epoch 19, Batch 130, Loss = 3.1960\n",
      "Epoch 19, Batch 140, Loss = 3.1966\n",
      "Epoch 19, Batch 150, Loss = 3.0695\n",
      "Epoch 19, Batch 160, Loss = 3.1712\n",
      "Epoch 19, Batch 170, Loss = 3.0984\n",
      "Epoch 19, Batch 180, Loss = 3.1755\n",
      "Epoch 19, Batch 190, Loss = 3.1535\n",
      "Epoch 19, Batch 200, Loss = 3.0728\n",
      "Epoch 19, Batch 210, Loss = 3.0438\n",
      "Epoch 19, Batch 220, Loss = 3.1897\n",
      "Epoch 19, Batch 230, Loss = 3.0828\n",
      "Epoch 19, Batch 240, Loss = 3.2584\n",
      "Epoch 19, Batch 250, Loss = 3.0319\n",
      "Epoch 19, Batch 260, Loss = 3.1334\n",
      "Epoch 19, Batch 270, Loss = 3.2557\n",
      "Epoch 19, Batch 280, Loss = 3.1771\n",
      "Epoch 19, Batch 290, Loss = 3.0613\n",
      "Epoch 19, Batch 300, Loss = 3.1440\n",
      "Epoch 19, Batch 310, Loss = 3.2060\n",
      "Epoch 19, Batch 320, Loss = 3.0430\n",
      "Epoch 19, Batch 330, Loss = 3.0497\n",
      "Epoch 19, Batch 340, Loss = 3.2005\n",
      "Epoch 19, Batch 350, Loss = 3.1541\n",
      "Epoch 19, Batch 360, Loss = 3.0965\n",
      "Epoch 19, Batch 370, Loss = 3.1948\n",
      "Epoch 19, Batch 380, Loss = 3.2274\n",
      "Epoch 19, Batch 390, Loss = 3.1620\n",
      "Epoch 19, Batch 400, Loss = 3.0889\n",
      "Epoch 19, Batch 410, Loss = 3.2329\n",
      "Epoch 19, Batch 420, Loss = 3.3717\n",
      "Epoch 19, Batch 430, Loss = 3.0694\n",
      "Epoch 19, Batch 440, Loss = 3.0786\n",
      "Epoch 19, Batch 450, Loss = 3.1381\n",
      "Epoch 19, Batch 460, Loss = 3.1548\n",
      "Epoch 19, Batch 470, Loss = 3.1434\n",
      "Epoch 19, Batch 480, Loss = 3.0879\n",
      "Epoch 19, Batch 490, Loss = 3.1543\n",
      "Epoch 19, Batch 500, Loss = 3.0017\n",
      "Epoch 19, Batch 510, Loss = 3.2018\n",
      "Epoch 19, Batch 520, Loss = 3.1337\n",
      "Epoch 19, Batch 530, Loss = 3.2708\n",
      "Epoch 19, Batch 540, Loss = 3.1985\n",
      "Epoch 19, Batch 550, Loss = 3.2117\n",
      "Epoch 19, Batch 560, Loss = 3.2181\n",
      "Epoch 19, Batch 570, Loss = 3.2128\n",
      "Epoch 19, Batch 580, Loss = 3.1718\n",
      "Epoch 19, Batch 590, Loss = 3.2297\n",
      "Epoch 19, Batch 600, Loss = 3.1178\n",
      "Epoch 19, Batch 610, Loss = 3.2619\n",
      "Epoch 19, Batch 620, Loss = 3.1371\n",
      "Epoch 19, Batch 630, Loss = 3.0849\n",
      "Epoch 19, Batch 640, Loss = 3.0936\n",
      "Epoch 19, Batch 650, Loss = 3.1850\n",
      "Epoch 19, Batch 660, Loss = 3.1555\n",
      "Epoch 19, Batch 670, Loss = 2.9524\n",
      "Epoch 19, Batch 680, Loss = 3.2764\n",
      "Epoch 19, Batch 690, Loss = 3.0526\n",
      "Epoch 19, Batch 700, Loss = 3.2694\n",
      "Epoch 19, Batch 710, Loss = 3.3326\n",
      "Epoch 19, Batch 720, Loss = 3.1152\n",
      "Epoch 19, Batch 730, Loss = 3.1345\n",
      "Epoch 19, Batch 740, Loss = 3.1300\n",
      "Epoch 19, Batch 750, Loss = 3.0992\n",
      "Epoch 19, Batch 760, Loss = 3.2986\n",
      "Epoch 19, Batch 770, Loss = 3.0572\n",
      "Epoch 19, Batch 780, Loss = 3.1413\n",
      "Epoch 19, Batch 790, Loss = 3.1013\n",
      "Epoch 19, Batch 800, Loss = 3.0097\n",
      "Epoch 19, Batch 810, Loss = 3.2393\n",
      "Epoch 19, Batch 820, Loss = 3.2209\n",
      "Epoch 19, Batch 830, Loss = 3.0574\n",
      "Epoch 19, Batch 840, Loss = 3.1912\n",
      "Epoch 19, Batch 850, Loss = 3.0672\n",
      "Epoch 19, Batch 860, Loss = 3.2578\n",
      "Epoch 19, Batch 870, Loss = 3.1292\n",
      "Epoch 19, Batch 880, Loss = 3.1377\n",
      "Epoch 19, Batch 890, Loss = 3.0970\n",
      "Epoch 19, Batch 900, Loss = 3.0901\n",
      "Epoch 19, Batch 910, Loss = 3.1456\n",
      "Epoch 19, Batch 920, Loss = 3.3070\n",
      "Epoch 19, Batch 930, Loss = 3.0989\n",
      "Epoch 19, Batch 940, Loss = 2.9696\n",
      "Epoch 19, Batch 950, Loss = 3.1130\n",
      "Epoch 19, Batch 960, Loss = 3.2831\n",
      "Epoch 19, Batch 970, Loss = 3.0490\n",
      "Epoch 19, Batch 980, Loss = 3.0838\n",
      "Epoch 19, Batch 990, Loss = 3.1577\n",
      "Epoch 19, Batch 1000, Loss = 3.2467\n",
      "Epoch 19, Batch 1010, Loss = 3.2590\n",
      "Epoch 19, Batch 1020, Loss = 3.0978\n",
      "Epoch 19, Batch 1030, Loss = 3.1455\n",
      "Epoch 19, Batch 1040, Loss = 3.1814\n",
      "Epoch 19, Batch 1050, Loss = 3.0047\n",
      "Epoch 19, Batch 1060, Loss = 2.9324\n",
      "Epoch 19, Batch 1070, Loss = 3.1852\n",
      "Epoch 19, Batch 1080, Loss = 3.1590\n",
      "Epoch 19, Batch 1090, Loss = 3.1584\n",
      "Epoch 19, Batch 1100, Loss = 3.2608\n",
      "Epoch 19, Batch 1110, Loss = 2.9881\n",
      "Epoch 19, Batch 1120, Loss = 3.1582\n",
      "Epoch 19, Average Loss = 3.1503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 21517 (\\N{CJK UNIFIED IDEOGRAPH-540D}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 23383 (\\N{CJK UNIFIED IDEOGRAPH-5B57}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 29983 (\\N{CJK UNIFIED IDEOGRAPH-751F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 25104 (\\N{CJK UNIFIED IDEOGRAPH-6210}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 35757 (\\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 32451 (\\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 25439 (\\N{CJK UNIFIED IDEOGRAPH-635F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "e:\\DL\\deeplearning\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "def train_name_generator(dat, dict, charset_size):\n",
    "    # 模型参数\n",
    "    vocab_size = charset_size\n",
    "    embed_size = 64\n",
    "    num_layers = 3\n",
    "    heads = 8\n",
    "    qk_dim = 64\n",
    "    ff_hidden_size = 128\n",
    "    dropout = 0.1\n",
    "    max_length = 10\n",
    "    \n",
    "    # 创建模型\n",
    "    model = NameGenerator(vocab_size, embed_size, num_layers, heads, qk_dim, \n",
    "                         ff_hidden_size, dropout, max_length)\n",
    "    \n",
    "    # 训练参数\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=charset_size-1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 训练循环\n",
    "    nepoch = 20\n",
    "    batch_size = 64\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # 随机打乱训练数据\n",
    "        train_indices = np.random.permutation(len(dat))\n",
    "        \n",
    "        for i in range(0, len(dat), batch_size):\n",
    "            batch_indices = train_indices[i:i+batch_size]\n",
    "            batch_names = [dat[idx] for idx in batch_indices]\n",
    "            \n",
    "            # 转换为tensor\n",
    "            input_tensor, actual_len, target_tensor = name_gen_names2tensor(batch_names, dict, charset_size)\n",
    "            \n",
    "            # 转换为模型输入格式 - 使用reshape避免内存布局问题\n",
    "            input_indices = torch.argmax(input_tensor, dim=-1).transpose(0, 1)  # (batch_size, seq_len)\n",
    "            target_indices = target_tensor.transpose(0, 1)  # (batch_size, seq_len)\n",
    "            \n",
    "            # 确保张量是连续的\n",
    "            input_indices = input_indices.contiguous()\n",
    "            target_indices = target_indices.contiguous()\n",
    "            \n",
    "            # 创建mask\n",
    "            seq_len = input_indices.size(1)\n",
    "            mask = torch.tril(torch.ones((1, seq_len, seq_len)))\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_indices, mask)\n",
    "            \n",
    "            # 计算损失 - 使用reshape而不是view\n",
    "            batch_size_actual = output.size(0)\n",
    "            seq_len_actual = output.size(1)\n",
    "            output_reshaped = output.reshape(batch_size_actual * seq_len_actual, charset_size)\n",
    "            target_reshaped = target_indices.reshape(batch_size_actual * seq_len_actual)\n",
    "            \n",
    "            loss = criterion(output_reshaped, target_reshaped)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            \n",
    "            if i // batch_size % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i // batch_size}, Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch}, Average Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"名字生成训练损失\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 测试名字生成训练函数\n",
    "print(\"开始训练名字生成器...\")\n",
    "try:\n",
    "    model = train_name_generator(dat, dict, charset_size)\n",
    "    print(\"训练完成！\")\n",
    "except Exception as e:\n",
    "    print(f\"训练过程中出现错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b480fc",
   "metadata": {},
   "source": [
    "## 名字生成和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec38988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成多个名字:\n",
      " 1. 夫拉斯特罗娃科特\n",
      " 2. 拉迪萨娃耶尔维奇\n",
      " 3. 巴马尔托夫斯基亚\n",
      " 4. 斯内亚克斯特内罗\n",
      " 5. 利尔基尼亚斯基斯\n",
      " 6. 普勒普特里什纳克\n",
      " 7. 科里奥特卡特贝克\n",
      " 8. 纳里亚尔迪诺夫斯\n",
      " 9. 勒纳斯蒂拉夫斯拉\n",
      "10. 帕里亚塔托夫斯卡\n",
      "\n",
      "==================================================\n",
      "名字生成测试:\n",
      "----------------------------------------\n",
      "起始字符: 亚 | 生成名字: 亚尼斯特拉克维奇\n",
      "起始字符: 马 | 生成名字: 马科尔迪拉蒂亚尼\n",
      "起始字符: 基 | 生成名字: 基拉伊纳克尔斯基\n",
      "起始字符: 维 | 生成名字: 维尔加斯基亚米内\n"
     ]
    }
   ],
   "source": [
    "# 名字生成和测试\n",
    "def generate_names(model, dict, num_names=10, max_len=8):\n",
    "    \"\"\"生成多个名字\"\"\"\n",
    "    model.eval()\n",
    "    generated_names = []\n",
    "    \n",
    "    # 获取姓氏\n",
    "    family_names = list(set([name[0] for name in dict if len(name) > 0]))\n",
    "    \n",
    "    for _ in range(num_names):\n",
    "        # 随机选择姓氏\n",
    "        start_char = np.random.choice(family_names)\n",
    "        generated_name = model.generate_name(start_char, dict, max_len)\n",
    "        generated_names.append(generated_name)\n",
    "    \n",
    "    return generated_names\n",
    "\n",
    "def test_name_generation(model, dict, max_len=8):\n",
    "    \"\"\"测试名字生成功能\"\"\"\n",
    "    model.eval()\n",
    "    test_start_chars = [\"亚\",\"马\",\"基\",\"维\"]\n",
    "    \n",
    "    print(\"名字生成测试:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for start_char in test_start_chars:\n",
    "        if start_char in dict:\n",
    "            generated_name = model.generate_name(start_char, dict, max_len)\n",
    "            print(f\"起始字符: {start_char} | 生成名字: {generated_name}\")\n",
    "        else:\n",
    "            print(f\"起始字符: {start_char} | 不在字典中\")\n",
    "\n",
    "# 运行名字生成测试\n",
    "try:\n",
    "    print(\"生成多个名字:\")\n",
    "    generated_names = generate_names(model, dict, num_names=10, max_len=8)\n",
    "    for i, name in enumerate(generated_names, 1):\n",
    "        print(f\"{i:2d}. {name}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    test_name_generation(model, dict, max_len=8)\n",
    "    \n",
    "except NameError:\n",
    "    print(\"模型未训练，无法进行名字生成测试\")\n",
    "    print(\"请先运行训练代码块\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
